---
title: "MAS-Thesis-SMA-Sentiment-Analysis"
author: "ajpucher"
date: "2018 M08 5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Inventory of all libs / load all libs here

```{r}
library(ggplot2)
library(twitteR)
library(reshape2)
library(dplyr)
library(magrittr)
library(tidytext)
library(ggrepel)
library(googleLanguageR)
library(stringr)
library(psych)
library(scatterplot3d)



```

### Source / load necessary R functions

```{r}
# Current working dir
getwd()

# Load authentication functions
source("../functions/auth.R")
# Load utils
source("../functions/util.R")

```

### Authentication to Twitter API (works, July 2018)

```{r}
# Start Authentication to Twitter API
auth_twitter()
# Start Authentication to Google NLP API
auth_google()

```

### Starting sentiment analysis, try some first examples
```{r}

load('../../data/output/smm-banks-fintech-output-descr-ratio-melted.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

#Some examples

#Trends in ZRH, Switzerland
#return data frame with name, country & woeid. 
Locs <- availableTrendLocations()
# Where woeid is a numerical identification code describing a location ID
# Filter the data frame for Delhi (India) and extract the woeid of the same
LocsCH = subset(Locs, country == "Switzerland") 
woeidZurich = subset(LocsCH, name == "Zurich")$woeid
# getTrends takes a specified woeid and returns the trending topics associated with that woeid
trends = getTrends(woeid=woeidZurich)
trends

# Search tweets about "Swiss Bank in a specific time range"
head(searchTwitter('Swiss Bank', since='2015-08-01', until= '2018-10-01'))

# Search for ubsschweiz tweets
ubsschweiz_tweets <- searchTwitter("#ubsschweiz", n=50)

zkbch_tweets <- searchTwitter("zkb_ch", n=50, lang="de")
str(zkbch_tweets)

zkbch_tweets

# Get user timelines
tweets = userTimeline("zkb_ch", n=50)
length(tweets)
tweetsDF_test <- twListToDF(tweets)
head(tweetsDF_test)
tweetsDF_test


```

## Tweet data loads from both sectors
### Loading tweet data from FinTech sector, lang=en
### This chunk can run any number of times and cyclical to append additional data
```{r}
# Load again initial file (before norm and ratio calculations)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

# Load FinTech data from HDD for an n-th run to append additional data 
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')


d.file

# Filter type "Fintech"
df_sa_fintech <- filter(d.file, type == "Fintech")
df_sa_fintech


# Subselect account name for Twitter search "tw_parse"
df_sub_sa_ft <- subset(df_sa_fintech, select=c("tw_parse"))
df_sub_sa_ft <- na.omit(df_sub_sa_ft)


# Create list of dataframes of each account - FINTECH
fintech_list = list()

for (i in 1:nrow(df_sub_sa_ft)) {
  print (i)
  
  tweets <- searchTwitter(df_sub_sa_ft$tw_parse[i], n=500, lang="en")
  
  if (length(tweets) != 0) {
    tweetsDF <- twListToDF(tweets) # Convert to DF
    tweetsDF$tw_parse <- df_sub_sa_ft$tw_parse[i] # Keep track from which account parsed
    fintech_list[[i]] <- tweetsDF # Add it to your list
  }
  
}

fintech_list

# Bind rows - bind both dataframes into one, 1. read df from hdd, 2. read df by API, bind them in one, save.
fintech_data_all_tweets <- dplyr::bind_rows(fintech_list, fintech_data_all_tweets)
# Save
save(fintech_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')

fintech_data_all_tweets

```

### Loading tweet data from banking sector, lang=de
### This chunk can run any number of times and cyclical to append additional data
```{r}
# Load again initial file (before norm and ratio calculations)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

# Load Bank data from HDD for an n-th run to append additional data 
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')

# Filter type "Bank"
df_sa_banks <- filter(d.file, type == "Bank")
df_sa_banks

df_sub_sa_banks <- subset(df_sa_banks, select=c("tw_parse"))
df_sub_sa_banks <- na.omit(df_sub_sa_banks)

# Create list of dataframes of each account - BANKS
banks_list = list()

for (i in 1:nrow(df_sub_sa_banks)) {
  print (i)
  
  tweets <- searchTwitter(df_sub_sa_banks$tw_parse[i], n=500, lang="de")
  
  if (length(tweets) != 0) {
    tweetsDF <- twListToDF(tweets) # Convert to DF
    tweetsDF$tw_parse <- df_sub_sa_banks$tw_parse[i] # Keep track from which account parsed
    banks_list[[i]] <- tweetsDF # Add it to your list
  }
  
}

# Bind rows - bind both dataframes into one, 1. read df from hdd, 2. read df by API, bind them in one, save.
banks_data_all_tweets <- dplyr::bind_rows(banks_list, banks_data_all_tweets)
# Save
save(banks_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')

banks_data_all_tweets


```


## Prototyping sentiment analysis
### Cleaning fintech data
### References:  https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}
# Load data
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')

glimpse(fintech_data_all_tweets)

#Created in the first tutorial
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")

#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy <- fintech_data_all_tweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

glimpse(ft_data_tidy)

```

### Create sentiment datasets (bing, nrc, nrc polarity, afinn)
### References: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}
ft_data_bing <- ft_data_tidy %>%
  inner_join(get_sentiments("bing"))

ft_data_nrc <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc"))

ft_data_nrc_sub <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", "negative"))

ft_data_afinn <- ft_data_tidy %>%
  inner_join(get_sentiments("afinn"))

```

### Plotting first sentiment analysis results
### References: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}

# Plotting word count
nrc_plot <- ft_data_nrc %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  #Use `fill = -word_count` to make the larger bars darker
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) + #Turn off the legend
  labs(x = NULL, y = "Word Count") +
  scale_y_continuous(limits = c(0, 3000)) + #Hard code the axis limit
  ggtitle("Entity Type Fintech: Word Count - NRC Sentiment") +
  coord_flip()
nrc_plot

ggsave("../../img/charts-sent-analysis/chart1_s-a-nrc-word-count.png")

# Plotting NRC sentiments with words, which triggered those sentiments
plot_words <- ft_data_nrc %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(8)) %>% #consider top_n() from dplyr also
  ungroup()

plot_words %>%
  #Set `y = 1` to just plot one variable and use word as the label
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  #You want the words, not the points
  geom_point(color = "transparent") +
  #Make sure the labels don't overlap
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.04,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Entity Type Fintech: Trigger Words - NRC Sentiment") +
  coord_flip()

ggsave("../../img/charts-sent-analysis/chart2_s-a-nrc-words-sentiments.png")

# Plotting AFINN histogram
ggplot(data=ft_data_afinn, aes(x=ft_data_afinn$score)) + 
  geom_histogram(breaks=seq(-10, 10, by=1), 
                 col="black", 
                 fill="blue", 
                 alpha = .2) + 
  labs(title="Entity Type Fintech: Histogram AFINN Senti-Score", x="Score", y="Frequency")

ggsave("../../img/charts-sent-analysis/chart3_s-a-afinn-hist-senti-score.png")

# SD, Mean, Median
ft_hist_afinn <- c(sd(ft_data_afinn$score), mean(ft_data_afinn$score), median(ft_data_afinn$score))
ft_hist_afinn


```


## H2 testing. H2: Companies in the Swiss FinTech sector have higher social impact (in comparison of followers to retweets) than Swiss banking companies.
### References: Adapted from Garcia (2018), Social Data Science Lecture
```{r}
# Load the saved file (the one with Twitter metadata)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')

d.file
# Cleaning: Only remove row, if tw_parse = NA
d.file <- d.file[!is.na(d.file$tw_parse), ]

# Add new avg. retweet column
d.file$avgRT <- rep(NA, length(d.file$tw_parse))

# Calculate for every entity (bank or fintech) the avg. amount of retweets and write it to a new column
### Attention for the Twitter API rate limit! If hit, wait and run again.
i = 1
for (ent in d.file$tw_parse) {
  print(i)
  tweets = try(userTimeline(d.file[i, 'tw_parse'], n = 100))
  if (length(tweets)<10)
    { d.file$avgRT[i] <- NA }
  else
    { tweetsDF <- twListToDF(tweets)
      d.file$avgRT[i] <- mean(tweetsDF$retweetCount)
    }
  i <- i+1
}

df_avgRT <- d.file

# Testing
tweets = try(userTimeline("ecollect", n = 100))
tweetsDF2<-twListToDF(tweets)
mean(tweetsDF2$retweetCount)

# Cleaning 2: Only remove row, if avgRT = NA
df_avgRT <- df_avgRT[!is.na(df_avgRT$avgRT), ]

# Save
save(df_avgRT, file = '../../data/output/smm-banks-fintech-output-descr-avg-retweets.Rdata')

```


### H2: Display histograms, build regression models for banking and fintech
```{r}
# Load the saved file (with avgRT - average retweets)
load('../../data/output/smm-banks-fintech-output-descr-avg-retweets.Rdata')

df_avgRT

# Filter type "Fintech"
df_avgRT_Fintech <- filter(df_avgRT, type == "Fintech")
df_avgRT_Fintech

# Filter type "Bank"
df_avgRT_Bank <- filter(df_avgRT, type == "Bank")
df_avgRT_Bank

# Show histogram on log scale, with Twitter followers
hist(log(df_avgRT_Bank$tw_followers))
hist(log(df_avgRT_Fintech$tw_followers))

# Plots
plot(log(df_avgRT_Bank$tw_followers), log(df_avgRT_Bank$avgRT), xlab="Log followers", ylab="Log SI")
plot(log(df_avgRT_Fintech$tw_followers), log(df_avgRT_Fintech$avgRT), xlab="Log followers", ylab="Log SI")

# Clean for zeros because of log
df_avgRT_Bank <-df_avgRT_Bank[df_avgRT_Bank$avgRT>0 & df_avgRT_Bank$tw_followers>0,]
df_avgRT_Fintech <-df_avgRT_Fintech[df_avgRT_Fintech$avgRT>0 & df_avgRT_Fintech$tw_followers>0,]

# Banks
df_avgRT_Bank$SI <- log(df_avgRT_Bank$avgRT)
df_avgRT_Bank$FC <- log(df_avgRT_Bank$tw_followers)
si_model_bank <- lm(SI ~ FC, data=df_avgRT_Bank)
summary(si_model_bank)


plot(log(df_avgRT_Bank$tw_followers), log(df_avgRT_Bank$avgRT), xlab="Log followers", ylab="Log SI")+
abline(si_model_bank$coefficients[1], si_model_bank$coefficients[2], col="red")

# Fintech
df_avgRT_Fintech$SI <- log(df_avgRT_Fintech$avgRT)
df_avgRT_Fintech$FC <- log(df_avgRT_Fintech$tw_followers)
si_model_fintech <- lm(SI ~ FC, data=df_avgRT_Fintech)
summary(si_model_fintech)

plot(log(df_avgRT_Fintech$tw_followers), log(df_avgRT_Fintech$avgRT), xlab="Log followers", ylab="Log SI")+
abline(si_model_fintech$coefficients[1], si_model_fintech$coefficients[2], col="red")


```


## H3 Testing: H3: Writing lengthy tweets contributes positively (in number of retweets and likes) to companies in the Swiss Fintech sector and Swiss banking sector.
### References:
### https://www.statmethods.net/stats/regression.html, https://www.statmethods.net/graphs/scatterplot.html
```{r}
# TODO. Here follows H3 testing. Measurement variables are: a. Message length, b. Message likes, c. Message retweets.
# 1. Corporate messages (tweets) will be collected, from both sectors Banks and FinTech.
# 2. Two each message a column will be added: Measurement variable a.: Message length
# 3. Two regression models will be build, for both sectors Banks and FinTech.

# Load FinTech data from HDD
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
# Load Bank data from HDD
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')

# 1. Retweets will be collected, from both sectors Banks and FinTech.
# Filter for retweets
# FINTECH
fintech_data_all_tweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")
fintech_data_only_tweets <- filter(fintech_data_all_tweets, isRetweet == "FALSE")

# BANKS
banks_data_all_tweets
banks_data_retweets <- filter(banks_data_all_tweets, isRetweet == "TRUE")
banks_data_only_tweets <- filter(banks_data_all_tweets, isRetweet == "FALSE")

# Add column msg / text length
fintech_data_only_tweets$text_length <- nchar(fintech_data_only_tweets$text)
banks_data_only_tweets$text_length <- nchar(banks_data_only_tweets$text)

# Build MLR model
# FINTECH
txtl_model_ft <- lm(text_length ~ favoriteCount + retweetCount, data=fintech_data_only_tweets)
summary(txtl_model_ft)
plot(txtl_model_ft)
txtl_model_ft

# 3D plot
attach(fintech_data_only_tweets) 
s3d_ft <-scatterplot3d(text_length, favoriteCount, retweetCount, pch=16, highlight.3d=TRUE,
  type="h", main="3D Scatterplot FinTech")

# BANKS
txtl_model_ba <- lm(text_length ~ favoriteCount + retweetCount, data=banks_data_only_tweets)
summary(txtl_model_ba)
plot(txtl_model_ba)

# 3D plot
attach(banks_data_only_tweets) 
s3d_ba <-scatterplot3d(text_length, favoriteCount, retweetCount, pch=16, highlight.3d=TRUE,
  type="h", main="3D Scatterplot Banks")



```


## H4 Testing: H4: Companies in the Swiss FinTech sector have a more positive polarity score measured on client responses to corporate tweets than Swiss banking companies.
### References: 
### CRAN package, R libs to access Google NLP AP: https://cran.r-project.org/web/packages/googleLanguageR/vignettes/nlp.html
### Data pre-cleaning: eBook, Mastering Social Media Mining in R (Ravindran, 2015), Cleaning the corpus
### Cloud Natural Language by Google: https://cloud.google.com/natural-language/
### https://taylorwhitten.github.io/blog/Google_NLP_SentimentAnalysis
```{r}
# Here follows H4 testing. 
# Measurement variables are: a. Polarity / Sentiment score of msg / tweet in (positive, neutral, negative: -1.0 to 1.0)

# Load FinTech data from HDD
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
# Load Bank data from HDD
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')

# 1. Retweets will be collected, from both sectors Banks and FinTech.
# Filter for retweets
# FINTECH
fintech_data_all_tweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")

#BANKS
banks_data_all_tweets
banks_data_retweets <- filter(banks_data_all_tweets, isRetweet == "TRUE")

# 2. Pre-cleaning and removing repetitive data, only tweets >= 5 chars
# FINTECH
ft_retweets_clean <- cleanTweets(fintech_data_retweets$text)
ft_retweets_clean = unique(ft_retweets_clean)
ft_retweets_clean <- subset(ft_retweets_clean, nchar(ft_retweets_clean)>= 5)
head (ft_retweets_clean)

# BANKS
ba_retweets_clean <- cleanTweets(banks_data_retweets$text)
ba_retweets_clean = unique(ba_retweets_clean)
ba_retweets_clean <- subset(ba_retweets_clean, nchar(ba_retweets_clean)>= 5)
head (ba_retweets_clean)

# 3. Polarity score will be calculated based on that tweets, by the help of Google NLP, an ML cloud-based approach.
# Google NLP API call, only of method: "language.documents.analyzeSentiment:	Analyzes the sentiment of the provided text."
# FINTECH
ft_nlp_result <- gl_nlp(ft_retweets_clean, nlp_type = "analyzeSentiment")
ft_nlp_result

# BANKS
ba_nlp_result <- gl_nlp(ba_retweets_clean, nlp_type = "analyzeSentiment")
ba_nlp_result

# 4. Visualization of results / histograms
# FINTECH
ggplot(ft_nlp_result$documentSentiment, aes(x=score, y=magnitude) ) +
  geom_point(alpha = 1/5) +
  geom_count()+
  geom_smooth(method = "loess", span=0.75) +
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis FinTech")

ggplot(ft_nlp_result$documentSentiment, aes( x=score) ) +
  geom_density(alpha = 0.25)+
  xlab("Polarity")+
  ylab("Magnitude")+
  ggtitle(paste0("Polarity Analysis FinTech"))
  

ft_nlp_result$documentSentiment

hist(ft_nlp_result$documentSentiment$magnitude, 
     main="Magnitude FinTech", 
     xlab="Magnitude", 
     las=1, 
     breaks=5)

hist(ft_nlp_result$documentSentiment$score, 
     main="Score FinTech", 
     xlab="Score", 
     las=1, 
     breaks=5)

# Create dataframe w. results, FinTech
df_ft_nlp_res <- ft_nlp_result$documentSentiment
df_ft_nlp_res$type <- rep("FinTech", length(df_ft_nlp_res$score))

df_ft_nlp_res

# BANKS
ggplot(ba_nlp_result$documentSentiment, aes(x=score, y=magnitude) ) +
  geom_point(alpha = 1/5) +
  geom_count()+
  geom_smooth(method = "loess", span=0.75) +
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis Banks")

ggplot(ba_nlp_result$documentSentiment, aes( x=score) ) +
  geom_density(alpha = 0.25)+
  xlab("Polarity")+
  ylab("Magnitude")+
  ggtitle(paste0("Polarity Analysis Banks"))
  

ba_nlp_result$documentSentiment

hist(ba_nlp_result$documentSentiment$magnitude, 
     main="Magnitude Banks", 
     xlab="Magnitude", 
     las=1, 
     breaks=5)

hist(ba_nlp_result$documentSentiment$score,
     main="Score Banks", 
     xlab="Score", 
     las=1, 
     breaks=5)

# Create dataframe w. results, Banks
df_ba_nlp_res <- ba_nlp_result$documentSentiment
df_ba_nlp_res$type <- rep("Banks", length(df_ba_nlp_res$score))

df_ba_nlp_res

# Create dataframe w. results, FinTech
df_nlp_plot_ba_ft <- rbind(df_ft_nlp_res, df_ba_nlp_res)
df_nlp_plot_ba_ft

# Creating FINAL H4 plot
ggplot(df_nlp_plot_ba_ft, aes(x=score, fill = factor(type)) ) +
  geom_density(alpha = 0.25)+
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis Banks and FinTech")+
  scale_fill_discrete(name="Entity Type")
  

# Using psych package for standard descriptive stats: https://www.statmethods.net/stats/descriptives.html
# Creating FINAL H4 descriptives
describe(ft_nlp_result$documentSentiment$score)
describe(ba_nlp_result$documentSentiment$score)

# Summary statistics
group_by(df_nlp_plot_ba_ft, type) %>%
  summarise(
    count = n(),
    mean = mean(score, na.rm = TRUE),
    median = median(score, na.rm = TRUE),
    sd = sd(score, na.rm = TRUE)
  )

# Shapiro test for normal distribution
with(df_nlp_plot_ba_ft, shapiro.test(score[type == "Banks"]))
with(df_nlp_plot_ba_ft, shapiro.test(score[type == "FinTech"]))

# Not normal distributed, so wilcoxon test
nlp_res_wilcox <- wilcox.test(score ~ type, data = df_nlp_plot_ba_ft,
                   exact = FALSE, alternative = "less")

nlp_res_wilcox

```

## Testing H5
### References:  https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}
# Load FinTech data from HDD
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
# Load Bank data from HDD
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')

glimpse(fintech_data_all_tweets)

#Created in the first tutorial
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")

#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy <- fintech_data_all_tweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

glimpse(ft_data_tidy)

# Create sentiment dataset nrc
ft_data_nrc <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc"))

# Plotting word count
nrc_plot <- ft_data_nrc %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  #Use `fill = -word_count` to make the larger bars darker
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) + #Turn off the legend
  labs(x = NULL, y = "Word Count") +
  scale_y_continuous(limits = c(0, 3000)) + #Hard code the axis limit
  ggtitle("Entity Type Fintech: Word Count - NRC Sentiment") +
  coord_flip()
nrc_plot

ft_data_nrc

# Plotting word emotion stats
ft_data_nrc %>%
  #filter(variable %in% "text") %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(x = NULL, y = "Word Count") +
  ggtitle("FinTech NRC Sentiment") +
  coord_flip()

```
