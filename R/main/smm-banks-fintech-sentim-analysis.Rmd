---
title: "MAS-Thesis-SMA-Sentiment-Analysis"
author: "ajpucher"
date: "2018 M08 5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Inventory of all libs / load all libs here

```{r}
library(circlize) # Visuals, circle diagrams, citation needed: Gu, Z. circlize implements and enhances circular visualization in R. Bioinformatics 2014.
library(dplyr) # Data ops
library(ggplot2) # Visuals
library(ggrepel) # Visuals
library(googleLanguageR) # Google NLP lib
library(magrittr) # Forward pipe operator
library(mvnormtest) # Tests for multivariate normality
library(parallel) # Parallelization / multi-core computation usage
library(psych) # Stats
library(RColorBrewer) # Colors
library(reshape2) # Reshape data
library(scatterplot3d) # 3d plots, MLR
library(stringr) # String ops
library(syuzhet) # Stanford NLP lib, citation needed
library(tidytext) # Text mining
library(twitteR) # Twitter API lib



```

### Source / load necessary R functions

```{r}
# Current working dir
getwd()

# turn-off scientific notation like 1e+48
options(scipen=999)

# Load authentication functions
source("../functions/auth.R")
# Load utils
source("../functions/util.R")

```

### Authentication to Twitter API (works, July 2018)

```{r}
# Start Authentication to Twitter API
auth_twitter()
# Start Authentication to Google NLP API
auth_google()

```

### Starting sentiment analysis, try some first examples
```{r}

load('../../data/output/smm-banks-fintech-output-descr-ratio-melted.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

#Some examples

#Trends in ZRH, Switzerland
#return data frame with name, country & woeid. 
Locs <- availableTrendLocations()
# Where woeid is a numerical identification code describing a location ID
# Filter the data frame for Delhi (India) and extract the woeid of the same
LocsCH = subset(Locs, country == "Switzerland") 
woeidZurich = subset(LocsCH, name == "Zurich")$woeid
# getTrends takes a specified woeid and returns the trending topics associated with that woeid
trends = getTrends(woeid=woeidZurich)
trends

# Search tweets about "Swiss Bank in a specific time range"
head(searchTwitter('Swiss Bank', since='2015-08-01', until= '2018-10-01'))

# Search for ubsschweiz tweets
ubsschweiz_tweets <- searchTwitter("#ubsschweiz", n=50)

zkbch_tweets <- searchTwitter("zkb_ch", n=50, lang="de")
str(zkbch_tweets)

zkbch_tweets

# Get user timelines
tweets = userTimeline("ubsschweiz", n=500)
length(tweets)
tweetsDF_test <- twListToDF(tweets)
head(tweetsDF_test)
tweetsDF_test


```

## Tweet data loads from both sectors
### Loading tweet data from FinTech sector, lang=en
### This chunk can run any number of times and cyclical to append additional data
```{r}
# Load again initial file (before norm and ratio calculations)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

# Load FinTech data from HDD for an n-th run to append additional data 
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')


d.file

# Filter type "Fintech"
df_sa_fintech <- filter(d.file, type == "Fintech")
df_sa_fintech


# Subselect account name for Twitter search "tw_parse"
df_sub_sa_ft <- subset(df_sa_fintech, select=c("tw_parse"))
df_sub_sa_ft <- na.omit(df_sub_sa_ft)


# Create list of dataframes of each account - FINTECH
fintech_list = list()

for (i in 1:nrow(df_sub_sa_ft)) {
  print (i)
  
  tweets <- searchTwitter(df_sub_sa_ft$tw_parse[i], n=500, lang="en")
  
  if (length(tweets) != 0) {
    tweetsDF <- twListToDF(tweets) # Convert to DF
    tweetsDF$tw_parse <- df_sub_sa_ft$tw_parse[i] # Keep track from which account parsed
    fintech_list[[i]] <- tweetsDF # Add it to your list
  }
  
}

fintech_list

# Bind rows - bind both dataframes into one, 1. read df from hdd, 2. read df by API, bind them in one, save.
fintech_data_all_tweets <- dplyr::bind_rows(fintech_list, fintech_data_all_tweets)
# Save
#save(fintech_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
save(fintech_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')

fintech_data_all_tweets

```

### Loading tweet data from banking sector, lang=de
### This chunk can run any number of times and cyclical to append additional data
```{r}
# Load again initial file (before norm and ratio calculations)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

# Load Bank data from HDD for an n-th run to append additional data 
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')

# Filter type "Bank"
df_sa_banks <- filter(d.file, type == "Bank")
df_sa_banks

df_sub_sa_banks <- subset(df_sa_banks, select=c("tw_parse"))
df_sub_sa_banks <- na.omit(df_sub_sa_banks)

# Create list of dataframes of each account - BANKS
banks_list = list()

for (i in 1:nrow(df_sub_sa_banks)) {
  print (i)
  
  tweets <- searchTwitter(df_sub_sa_banks$tw_parse[i], n=500, lang="de")
  
  if (length(tweets) != 0) {
    tweetsDF <- twListToDF(tweets) # Convert to DF
    tweetsDF$tw_parse <- df_sub_sa_banks$tw_parse[i] # Keep track from which account parsed
    banks_list[[i]] <- tweetsDF # Add it to your list
  }
  
}

# Bind rows - bind both dataframes into one, 1. read df from hdd, 2. read df by API, bind them in one, save.
banks_data_all_tweets <- dplyr::bind_rows(banks_list, banks_data_all_tweets)
# Save
#save(banks_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')
save(banks_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

banks_data_all_tweets


```


## Prototyping sentiment analysis
### Cleaning fintech data
### References:  https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}
# Load data
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')

glimpse(fintech_data_all_tweets)

#Created in the first tutorial
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")

#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy <- fintech_data_all_tweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

glimpse(ft_data_tidy)

```

### Create sentiment datasets (bing, nrc, nrc polarity, afinn)
### References: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}
ft_data_bing <- ft_data_tidy %>%
  inner_join(get_sentiments("bing"))

ft_data_nrc <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc"))

ft_data_nrc_sub <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", "negative"))

ft_data_afinn <- ft_data_tidy %>%
  inner_join(get_sentiments("afinn"))

```

### Plotting first sentiment analysis results
### References: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}

# Plotting word count
nrc_plot <- ft_data_nrc %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  #Use `fill = -word_count` to make the larger bars darker
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) + #Turn off the legend
  labs(x = NULL, y = "Word Count") +
  scale_y_continuous(limits = c(0, 3000)) + #Hard code the axis limit
  ggtitle("Entity Type Fintech: Word Count - NRC Sentiment") +
  coord_flip()
nrc_plot

ggsave("../../img/charts-sent-analysis/chart1_s-a-nrc-word-count.png")

# Plotting NRC sentiments with words, which triggered those sentiments
plot_words <- ft_data_nrc %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(8)) %>% #consider top_n() from dplyr also
  ungroup()

plot_words %>%
  #Set `y = 1` to just plot one variable and use word as the label
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  #You want the words, not the points
  geom_point(color = "transparent") +
  #Make sure the labels don't overlap
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.04,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Entity Type Fintech: Trigger Words - NRC Sentiment") +
  coord_flip()

ggsave("../../img/charts-sent-analysis/chart2_s-a-nrc-words-sentiments.png")

# Plotting AFINN histogram
ggplot(data=ft_data_afinn, aes(x=ft_data_afinn$score)) + 
  geom_histogram(breaks=seq(-10, 10, by=1), 
                 col="black", 
                 fill="blue", 
                 alpha = .2) + 
  labs(title="Entity Type Fintech: Histogram AFINN Senti-Score", x="Score", y="Frequency")

ggsave("../../img/charts-sent-analysis/chart3_s-a-afinn-hist-senti-score.png")

# SD, Mean, Median
ft_hist_afinn <- c(sd(ft_data_afinn$score), mean(ft_data_afinn$score), median(ft_data_afinn$score))
ft_hist_afinn


```


## H2 testing. H2: Companies in the Swiss FinTech sector have a higher social impact in comparison of followers to retweets than Swiss banking companies.
### References: Adapted from Garcia (2018), Social Data Science Lecture
```{r}
# Load the saved file (the one with Twitter metadata)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')

d.file
# Cleaning: Only remove row, if tw_parse = NA
d.file <- d.file[!is.na(d.file$tw_parse), ]

# Add new avg. retweet column
d.file$avgRT <- rep(NA, length(d.file$tw_parse))

# Calculate for every entity (bank or fintech) the avg. amount of retweets and write it to a new column
### Attention for the Twitter API rate limit! If hit, wait and run again.
i = 1
for (ent in d.file$tw_parse) {
  print(i)
  tweets = try(userTimeline(d.file[i, 'tw_parse'], n = 100))
  if (length(tweets)<10)
    { d.file$avgRT[i] <- NA }
  else
    { tweetsDF <- twListToDF(tweets)
      d.file$avgRT[i] <- mean(tweetsDF$retweetCount)
    }
  i <- i+1
}

df_avgRT <- d.file

# Testing
tweets = try(userTimeline("ecollect", n = 100))
tweetsDF2<-twListToDF(tweets)
mean(tweetsDF2$retweetCount)

# Cleaning 2: Only remove row, if avgRT = NA
df_avgRT <- df_avgRT[!is.na(df_avgRT$avgRT), ]

# Save
#save(df_avgRT, file = '../../data/output/smm-banks-fintech-output-descr-avg-retweets.Rdata')
save(df_avgRT, file = '../../data/output/smm-banks-fintech-output-descr-avg-retweets2.Rdata')

```


### H2: Display histograms, build regression models for banking and fintech
### Interpreting beta coefficients, linear regression etc. http://r-statistics.co/Linear-Regression.html
```{r}
# Load the saved file (with avgRT - average retweets)
load('../../data/output/smm-banks-fintech-output-descr-avg-retweets.Rdata')

df_avgRT

# Filter type "Fintech"
df_avgRT_Fintech <- filter(df_avgRT, type == "Fintech")
df_avgRT_Fintech

# Filter type "Bank"
df_avgRT_Bank <- filter(df_avgRT, type == "Bank")
df_avgRT_Bank

# Show histogram on log scale, with Twitter followers
hist(log(df_avgRT_Bank$tw_followers))
hist(log(df_avgRT_Fintech$tw_followers))

# Plot histogram banking with percentage
png(file="../../img/charts-histogram/hist_ba_h2.png")
hist_ba_h2 = hist(log(df_avgRT_Bank$tw_followers), plot = FALSE)
hist_ba_h2$density = hist_ba_h2$counts/sum(hist_ba_h2$counts)
plot(hist_ba_h2, freq = FALSE,  xlab="Followers (log scale)", ylab="Percentage", main="Histogram of followers - Banking")
dev.off()

# Plot histogram FinTech with percentage
png(file="../../img/charts-histogram/hist_ft_h2.png")
hist_ft_h2 = hist(log(df_avgRT_Fintech$tw_followers), plot = FALSE)
hist_ft_h2$density = hist_ft_h2$counts/sum(hist_ft_h2$counts)
plot(hist_ft_h2, freq = FALSE,  xlab="Followers (log scale)", ylab="Percentage", main="Histogram of followers - FinTech")
dev.off()

# Descriptive stats
describe(df_avgRT_Bank$tw_followers)
describe(df_avgRT_Fintech$tw_followers)

# Plots
plot(log(df_avgRT_Bank$tw_followers), log(df_avgRT_Bank$avgRT), xlab="Log followers", ylab="Log SI")
plot(log(df_avgRT_Fintech$tw_followers), log(df_avgRT_Fintech$avgRT), xlab="Log followers", ylab="Log SI")

# Clean for zeros because of logarithm
df_avgRT_Bank <-df_avgRT_Bank[df_avgRT_Bank$avgRT>0 & df_avgRT_Bank$tw_followers>0,]
df_avgRT_Fintech <-df_avgRT_Fintech[df_avgRT_Fintech$avgRT>0 & df_avgRT_Fintech$tw_followers>0,]

# Banks
df_avgRT_Bank$SI <- log(df_avgRT_Bank$avgRT)
df_avgRT_Bank$FC <- log(df_avgRT_Bank$tw_followers)
si_model_bank <- lm(SI ~ FC, data=df_avgRT_Bank)
summary(si_model_bank)


plot(log(df_avgRT_Bank$tw_followers), log(df_avgRT_Bank$avgRT), xlab="Log followers", ylab="Log SI", main = "Banks")+
abline(si_model_bank$coefficients[1], si_model_bank$coefficients[2], col="red")

# Fintech
df_avgRT_Fintech$SI <- log(df_avgRT_Fintech$avgRT)
df_avgRT_Fintech$FC <- log(df_avgRT_Fintech$tw_followers)
si_model_fintech <- lm(SI ~ FC, data=df_avgRT_Fintech)
summary(si_model_fintech)

plot(log(df_avgRT_Fintech$tw_followers), log(df_avgRT_Fintech$avgRT), xlab="Log followers", ylab="Log SI", main = "FinTech")+
abline(si_model_fintech$coefficients[1], si_model_fintech$coefficients[2], col="red")


```


## H3 Testing: H3: Tweet length has an impact to the number of retweets and likes to companies in the Swiss banking and Swiss FinTech sector.
### References:
### https://www.statmethods.net/graphs/scatterplot.html
### http://www.sthda.com/english/wiki/manova-test-in-r-multivariate-analysis-of-variance
### https://www.statmethods.net/stats/anova.html
### https://www.statmethods.net/stats/anovaAssumptions.html
```{r}
# Here follows H3 testing. Measurement variables are: a. Message length, b. Message likes, c. Message retweets.
# 1. Corporate messages (tweets) will be collected, from both sectors Banks and FinTech.
# 2. Two each message a column will be added: Measurement variable a.: Message length
# 3. Two MANOVA (with 2 DVs, 1 IV) regression models will be build, for both sectors Banks and FinTech.

# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')
# Load Bank data from HDD
#load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

# 1. Retweets will be collected, from both sectors Banks and FinTech.
# Filter for retweets
# FINTECH
fintech_data_all_tweets
fintech_data_only_tweets <- filter(fintech_data_all_tweets, isRetweet == "FALSE")

# BANKS
banks_data_all_tweets
banks_data_only_tweets <- filter(banks_data_all_tweets, isRetweet == "FALSE")

# Add column msg / text length
fintech_data_only_tweets$text_length <- nchar(fintech_data_only_tweets$text)
banks_data_only_tweets$text_length <- nchar(banks_data_only_tweets$text)

# Descriptive stats
describe(fintech_data_only_tweets$text_length)
describe(banks_data_only_tweets$text_length)

# QQ-plots for showing correlation
qqplot(fintech_data_only_tweets$text_length, fintech_data_only_tweets$favoriteCount)
qqplot(fintech_data_only_tweets$text_length, fintech_data_only_tweets$retweetCount)
qqplot(banks_data_only_tweets$text_length, banks_data_only_tweets$favoriteCount)
qqplot(banks_data_only_tweets$text_length, banks_data_only_tweets$retweetCount)

# Histograms about text_length
hist(fintech_data_only_tweets$text_length)
hist(banks_data_only_tweets$text_length)

# Plot histogram banking with percentage
png(file="../../img/charts-histogram/hist_ba_h3.png")
hist_ba_h3 = hist(banks_data_only_tweets$text_length, plot = FALSE)
hist_ba_h3$density = hist_ba_h3$counts/sum(hist_ba_h3$counts)
plot(hist_ba_h3, freq = FALSE,  xlab="Tweet length", ylab="Percentage", main="Histogram of tweet length - Banking")
dev.off()

# Plot histogram FinTech with percentage
png(file="../../img/charts-histogram/hist_ft_h3.png")
hist_ft_h3 = hist(fintech_data_only_tweets$text_length, plot = FALSE)
hist_ft_h3$density = hist_ft_h3$counts/sum(hist_ft_h3$counts)
plot(hist_ft_h3, freq = FALSE,  xlab="Tweet length", ylab="Percentage", main="Histogram of tweet length - FinTech")
dev.off()


# Validation for multivariate tests, Shapiro-Wilk test
# BANKS
# Subsetting the 2 DVs
ba_shap_dvs <- dplyr::select(banks_data_only_tweets, favoriteCount, retweetCount)
ba_shap_dvs <- as.matrix(ba_shap_dvs) # n x p numeric matrix of the 2 DVs

# Test Multivariate Normality for the 2 DVs
M <- t(ba_shap_dvs)
mshapiro.test(M)
# p < 0.00

# FINTECH
# Subsetting the 2 DVs
ft_shap_dvs <- dplyr::select(fintech_data_only_tweets, favoriteCount, retweetCount)
ft_shap_dvs <- as.matrix(ft_shap_dvs) # n x p numeric matrix of the 2 DVs

# Test Multivariate Normality for the 2 DVs
M <- t(ft_shap_dvs)
mshapiro.test(M)
# p < 0.00

# Do the MANOVA test, FINTECH
ft_res_man <- manova(cbind(favoriteCount, retweetCount) ~ text_length, data = fintech_data_only_tweets)
summary(ft_res_man)
# Investigate which variables are significant
summary.aov(ft_res_man)

# Do the MANOVA test, BANKS
ba_res_man <- manova(cbind(favoriteCount, retweetCount) ~ text_length, data = banks_data_only_tweets)
summary(ba_res_man)
# Investigate which variables are significant
summary.aov(ba_res_man)

# Plotting
# 3D plot FINTECH
attach(fintech_data_only_tweets) 
s3d_ft <-scatterplot3d(text_length, favoriteCount, retweetCount, pch=16, highlight.3d=TRUE,
  type="h", main="3D Scatterplot FinTech", , xlab ="Tweet length", ylab = "Favorites", zlab = "Retweets", col.axis = "gray", col.grid = "gray")

# 3D plot BANKS
attach(banks_data_only_tweets) 
s3d_ba <-scatterplot3d(text_length, favoriteCount, retweetCount, pch=16, highlight.3d=TRUE,
  type="h", main="3D Scatterplot Banks", , xlab ="Tweet length", ylab = "Favorites", zlab = "Retweets", col.axis = "gray", col.grid = "gray")



```


## H4 Testing: H4: Companies in the Swiss FinTech sector have a higher polarity score based on retweets to corporate tweets than Swiss banking companies.
### References: 
### CRAN package, R libs to access Google NLP AP: https://cran.r-project.org/web/packages/googleLanguageR/vignettes/nlp.html
### Data pre-cleaning: eBook, Mastering Social Media Mining in R (Ravindran, 2015), Cleaning the corpus
### Cloud Natural Language by Google: https://cloud.google.com/natural-language/, https://taylorwhitten.github.io/blog/Google_NLP_SentimentAnalysis
```{r}
# Here follows H4 testing. 
# Measurement variables are: a. Polarity / Sentiment score of msg / tweet in (positive, neutral, negative: -1.0 to 1.0)

# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')
# Load Bank data from HDD
#load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

# 1. Retweets will be collected, from both sectors Banks and FinTech.
# Filter for retweets
# FINTECH
fintech_data_all_tweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")

#BANKS
banks_data_all_tweets
banks_data_retweets <- filter(banks_data_all_tweets, isRetweet == "TRUE")

# 2. Pre-cleaning and removing repetitive data, only tweets >= 5 chars
# FINTECH
ft_retweets_clean <- cleanTweets(fintech_data_retweets$text)
ft_retweets_clean = unique(ft_retweets_clean)
ft_retweets_clean <- subset(ft_retweets_clean, nchar(ft_retweets_clean)>= 5)
head (ft_retweets_clean)

# BANKS
ba_retweets_clean <- cleanTweets(banks_data_retweets$text)
ba_retweets_clean = unique(ba_retweets_clean)
ba_retweets_clean <- subset(ba_retweets_clean, nchar(ba_retweets_clean)>= 5)
head (ba_retweets_clean)

# 3. Polarity score will be calculated based on that tweets, by the help of Google NLP, an ML cloud-based approach.
# Google NLP API call, only of method: "language.documents.analyzeSentiment:	Analyzes the sentiment of the provided text."
# FINTECH
ft_nlp_result <- gl_nlp(ft_retweets_clean, nlp_type = "analyzeSentiment")
ft_nlp_result
# SAVE FINTECH Google NLP results
save(ft_nlp_result, file = '../../data/output/smm-banks-fintech-output-fintech-google-nlp-scores.Rdata')
# LOAD again (restore)
load('../../data/output/smm-banks-fintech-output-fintech-google-nlp-scores.Rdata')

# BANKS
ba_nlp_result <- gl_nlp(ba_retweets_clean, nlp_type = "analyzeSentiment")
ba_nlp_result
# SAVE BANKS Google NLP results
save(ba_nlp_result, file = '../../data/output/smm-banks-fintech-output-banks-google-nlp-scores.Rdata')
# LOAD again (restore)
load('../../data/output/smm-banks-fintech-output-banks-google-nlp-scores.Rdata')


# 4. Visualization of results / histograms
# FINTECH
ggplot(ft_nlp_result$documentSentiment, aes(x=score, y=magnitude) ) +
  geom_point(alpha = 1/5) +
  geom_count()+
  geom_smooth(method = "loess", span=0.75) +
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis FinTech")

ggplot(ft_nlp_result$documentSentiment, aes( x=score) ) +
  geom_density(alpha = 0.25)+
  xlab("Polarity")+
  ylab("Magnitude")+
  ggtitle(paste0("Polarity Analysis FinTech"))
  

ft_nlp_result$documentSentiment

hist(ft_nlp_result$documentSentiment$magnitude, 
     main="Magnitude FinTech", 
     xlab="Magnitude", 
     las=1, 
     breaks=5)

hist(ft_nlp_result$documentSentiment$score, 
     main="Score FinTech", 
     xlab="Score", 
     las=1, 
     breaks=5)


# Plot histogram FinTech with percentage
png(file="../../img/charts-histogram/hist_ft_h4.png")
hist_ft_h4 = hist(ft_nlp_result$documentSentiment$score, plot = FALSE)
hist_ft_h4$density = hist_ft_h4$counts/sum(hist_ft_h4$counts)
plot(hist_ft_h4, freq = FALSE,  xlab="Sentiment score", ylab="Percentage", main="Histogram of sentiment score - FinTech")
dev.off()


# Create dataframe w. results, FinTech
df_ft_nlp_res <- ft_nlp_result$documentSentiment
df_ft_nlp_res$type <- rep("FinTech", length(df_ft_nlp_res$score))
# Additionally, add the retweets
df_ft_nlp_res$retweet <- ft_retweets_clean
df_ft_nlp_res

# BANKS
ggplot(ba_nlp_result$documentSentiment, aes(x=score, y=magnitude) ) +
  geom_point(alpha = 1/5) +
  geom_count()+
  geom_smooth(method = "loess", span=0.75) +
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis Banks")

ggplot(ba_nlp_result$documentSentiment, aes( x=score) ) +
  geom_density(alpha = 0.25)+
  xlab("Polarity")+
  ylab("Magnitude")+
  ggtitle(paste0("Polarity Analysis Banks"))
  

ba_nlp_result$documentSentiment

hist(ba_nlp_result$documentSentiment$magnitude, 
     main="Magnitude Banks", 
     xlab="Magnitude", 
     las=1, 
     breaks=5)

hist(ba_nlp_result$documentSentiment$score,
     main="Score Banks", 
     xlab="Score", 
     las=1, 
     breaks=5)

# Plot histogram Banks with percentage
png(file="../../img/charts-histogram/hist_ba_h4.png")
hist_ba_h4 = hist(ba_nlp_result$documentSentiment$score, plot = FALSE)
hist_ba_h4$density = hist_ba_h4$counts/sum(hist_ba_h4$counts)
plot(hist_ba_h4, freq = FALSE,  xlab="Sentiment score", ylab="Percentage", main="Histogram of sentiment score - Banks")
dev.off()


# Create dataframe w. results, Banks
df_ba_nlp_res <- ba_nlp_result$documentSentiment
df_ba_nlp_res$type <- rep("Banks", length(df_ba_nlp_res$score))
# Additionally, add the retweets
df_ba_nlp_res$retweet <- ba_retweets_clean

df_ba_nlp_res

# Create dataframe w. results, FinTech
df_nlp_plot_ba_ft <- rbind(df_ft_nlp_res, df_ba_nlp_res)
df_nlp_plot_ba_ft

# Creating FINAL H4 plot
ggplot(df_nlp_plot_ba_ft, aes(x=score, y=magnitude, fill = factor(type)) ) +
  geom_density(alpha = 0.25)+
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis Banks and FinTech")+
  scale_fill_discrete(name="Entity Type")

# Chart: Boxplot Sentiment Score Compared to Entity Type Total
ggplot(df_nlp_plot_ba_ft) +
  geom_boxplot(aes(type, (score), col=as.factor(type))) +
  labs(x = "Type", y = "Sentiment score",
       title="Sentiment Score Compared to Entity Type")+
  scale_color_discrete(name="Entity Type")
  

# Using psych package for standard descriptive stats: https://www.statmethods.net/stats/descriptives.html
# Creating FINAL H4 descriptives
describe(ft_nlp_result$documentSentiment$score)
describe(ba_nlp_result$documentSentiment$score)

# Summary statistics
group_by(df_nlp_plot_ba_ft, type) %>%
  summarise(
    count = n(),
    mean = mean(score, na.rm = TRUE),
    median = median(score, na.rm = TRUE),
    sd = sd(score, na.rm = TRUE)
  )

# Shapiro test for normal distribution
with(df_nlp_plot_ba_ft, shapiro.test(score[type == "Banks"]))
with(df_nlp_plot_ba_ft, shapiro.test(score[type == "FinTech"]))

# Not normal distributed, so Wilcoxon rank-sum test
nlp_res_wilcox <- wilcox.test(score ~ type, data = df_nlp_plot_ba_ft,
                   exact = FALSE)

nlp_res_wilcox

```

## Testing H5. H5: Companies in the Swiss FinTech sector experience higher trust based on retweets to corporate tweets than Swiss banking companies.
### References:  https://www.datacamp.com/community/tutorials/sentiment-analysis-R
### FinTech - sentiment analysis mostly in EN
```{r}
# FINTECH
# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')

# Load ALL TWEETS, Capture retweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")
glimpse(fintech_data_retweets)
```

### FinTech - Initially try the tidytext method
```{r}
# 1.A. Prep data to tidy text format, undertake sentiment analysis based on individual words, use tidytext libs
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy <- fintech_data_retweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

glimpse(ft_data_tidy)

# 2.A. TIDYTEXT: Undertake sentiment analysis with built-in tidytext nrc methodology
# Create sentiment dataset nrc
ft_data_nrc <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc"))

ft_data_nrc

# 3.A. Plotting word emotion stats
ft_data_nrc %>%
  #filter(variable %in% "text") %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(x = NULL, y = "Word Count") +
  ggtitle("FinTech NRC Sentiment") +
  coord_flip()

ft_data_tidy$word

# 4.A. EXTRA Plotting
# Plotting NRC sentiments with words, which triggered those sentiments
plot_words <- ft_data_nrc %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(8)) %>% #consider top_n() from dplyr also
  ungroup()

plot_words %>%
  #Set `y = 1` to just plot one variable and use word as the label
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  #You want the words, not the points
  geom_point(color = "transparent") +
  #Make sure the labels don't overlap
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.04,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Entity Type Fintech: Trigger Words - NRC Sentiment") +
  coord_flip()

# MOOD RING plotting
# Calc. NRC for all tweets
# 1. Prep data to tidy text format, undertake sentiment analysis based on individual words, use tidytext libs
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy_all <- fintech_data_all_tweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

# 2.TIDYTEXT: Undertake sentiment analysis with built-in tidytext nrc methodology
# Create sentiment dataset nrc
ft_data_nrc_all <- ft_data_tidy_all %>%
  inner_join(get_sentiments("nrc"))

# Plot the mood ring! :-)
#Define some colors to use throughout
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00", "#D65E00", "#6C5B7B", "#355C7D")
grid.col = c("TRUE" = "grey", "FALSE" = "grey", "anger" = my_colors[1], "anticipation" = my_colors[2], "disgust" = my_colors[3], "fear" = my_colors[4], "joy" = my_colors[5], "sadness" = my_colors[6], "surprise" = my_colors[7], "trust" = my_colors[8])

tweet_mood <-  ft_data_nrc_all %>%
  filter(!sentiment %in% c("positive", "negative")) %>%
  count(sentiment, isRetweet) %>%
  group_by(sentiment, isRetweet) %>%
  summarise(sentiment_sum = sum(n)) %>%
  ungroup()

circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(tweet_mood[[1]])) - 1), 15,
                         rep(5, length(unique(tweet_mood[[2]])) - 1), 15))
chordDiagram(tweet_mood, grid.col = grid.col, transparency = .2)
title("FinTech Sector: Relationship Between Mood and Tweets")
```

### FinTech - SYUZHET method (The SELECTED one), here starts the official H5 testing
```{r}

# FINTECH
# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')

# Load ALL TWEETS, Capture retweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")
glimpse(fintech_data_retweets)

# --- OTHER METHODOLOGY: SYUZHET ---
# 1.B. SYUZHET: Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ft_retweets_clean <- cleanTweets(fintech_data_retweets$text)
ft_retweets_clean = unique(ft_retweets_clean) # del redundant tweets
head (ft_retweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ft_data_nrc_syu <- get_nrc_sentiment(ft_retweets_clean, language = "english")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ft_data_nrc_syu <- get_nrc_sentiment(ft_retweets_clean, language = "english", cl=cl)
stopCluster(cl)

# 2.B. RETWEETS Plotting
# Plots, SENTENCES (whole tweets, sentences)
barplot(
  sort(colSums(prop.table(ft_data_nrc_syu[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1,
  col=c("white"),
  main = "Emotional dimensions - FinTech", xlab="Percentage"
)

# 3.B. EXTRA Plotting
# Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ft_alltweets_clean <- cleanTweets(fintech_data_all_tweets$text)
head (ft_alltweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ft_data_nrc_syu_all <- get_nrc_sentiment(ft_alltweets_clean, language = "english")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ft_data_nrc_syu_all <- get_nrc_sentiment(ft_alltweets_clean, language = "english", cl=cl)
stopCluster(cl)

# Merging to a big dataset, FinTech syuzhet
ft_big_syu <- cbind(ft_data_nrc_syu_all, ft_alltweets_clean)
ft_big_syu <- cbind(ft_big_syu, fintech_data_all_tweets)

# Further cleaning - DEL redundant records, if possible
ft_big_syu = unique(ft_big_syu) # del redundant tweets

# SAVE whole FinTech dataset with cleaned tweets and syuzhet NLP results
save(ft_big_syu, file = '../../data/output/smm-banks-fintech-output-fintech-syu-nlp.Rdata')
# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-fintech-syu-nlp.Rdata')

# Only select needed data for FinTech mood ring
ft_fin_syu <- dplyr::select(ft_big_syu, isRetweet, anger, anticipation, disgust, fear, joy, sadness, surprise, trust)
ft_fin_syu

# Data prep for mood ring - data melt
ft_melt_syu <- melt(data = ft_fin_syu, id.vars = c("isRetweet"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

# Data aggregation
ft_mood <-  ft_melt_syu %>%
  filter(!value == 0) %>% # important! it should not count zeros
  count(isRetweet, variable) %>%
  group_by(isRetweet, variable) %>%
  summarise(variable_sum = sum(n)) %>%
  arrange((variable)) %>%
  ungroup()

ft_mood <- ft_mood[c("variable", "isRetweet", "variable_sum")]

# Rename column values isRetweet = false -> TWEET, isRetweet = true -> RETWEET
ft_mood$isRetweet[ft_mood$isRetweet == "FALSE"]  <- c("TWEET")
ft_mood$isRetweet[ft_mood$isRetweet == "TRUE"]   <- c("RETWEET") 


# Plot the final FINTECH mood ring in extra high quality, vector graphics SVG format!
svg(file="../../img/charts-sentiment/ft-moodring.svg",width=5,height=5, pointsize=10)
# Colors are tried to define them accordingly to Plutchik´s color wheel (2001) to represent the emotions.
grid.col = c("TWEET" = "aliceblue", "RETWEET" = "aliceblue", "anger" = "orangered2", "anticipation" = "orange2", "disgust" = "mediumpurple2", "fear" = "green4", "joy" = "gold2", "sadness" = "deepskyblue2", "surprise" = "dodgerblue2", "trust" = "olivedrab3")

circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(ft_mood[[1]])) - 1), 15,
                         rep(5, length(unique(ft_mood[[2]])) - 1), 15))
chordDiagram(ft_mood, grid.col = grid.col, transparency = .2)
title("FinTech: Emotional Relationships - Tweets and Retweets")
dev.off()

```


### Banking - sentiment analysis mostly in DE
### References:
### tidytext, circlize: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
### syuzhet package (Stanford NLP): https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html
### German stopwords: https://www.inwt-statistics.de/blog-artikel-lesen/text-mining-part-2-making-of.html
```{r}
# BANK
# Load Bank data from HDD
#load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

# Load ALL TWEETS, Capture retweets
banks_data_retweets <- filter(banks_data_all_tweets, isRetweet == "TRUE")

# 1. SYUZHET: Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ba_retweets_clean <- cleanTweets(banks_data_retweets$text)
ba_retweets_clean = unique(ba_retweets_clean) # del redundant tweets
head (ba_retweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ba_data_nrc <- get_nrc_sentiment(ba_retweets_clean, language = "german")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ba_data_nrc <- get_nrc_sentiment(ba_retweets_clean, language = "german", cl=cl)
stopCluster(cl)

# 2. Plotting
# Plots, SENTENCES (whole tweets, sentences)
barplot(
  sort(colSums(prop.table(ba_data_nrc[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1,
  col= c("white"),
  main = "Emotional dimensions - Banking", xlab="Percentage"
)


# 3.B. EXTRA Plotting
# Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ba_alltweets_clean <- cleanTweets(banks_data_all_tweets$text)
head (ba_alltweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ba_data_nrc_syu_all <- get_nrc_sentiment(ba_alltweets_clean, language = "german")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ba_data_nrc_syu_all <- get_nrc_sentiment(ba_alltweets_clean, language = "german", cl=cl)
stopCluster(cl)

# Merging to a big dataset, Banks syuzhet
ba_big_syu <- cbind(ba_data_nrc_syu_all, ba_alltweets_clean)
ba_big_syu <- cbind(ba_big_syu, banks_data_all_tweets)

# Further cleaning - DEL redundant records, if possible
ba_big_syu = unique(ba_big_syu) # del redundant tweets

# SAVE whole Bank dataset with cleaned tweets and syuzhet NLP results
save(ba_big_syu, file = '../../data/output/smm-banks-fintech-output-banks-syu-nlp.Rdata')
# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-banks-syu-nlp.Rdata')

# Only select needed data for Banks mood ring
ba_fin_syu <- dplyr::select(ba_big_syu, isRetweet, anger, anticipation, disgust, fear, joy, sadness, surprise, trust)
ba_fin_syu

# Data prep for mood ring - data melt
ba_melt_syu <- melt(data = ba_fin_syu, id.vars = c("isRetweet"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

# Data aggregation
ba_mood <-  ba_melt_syu %>%
  filter(!value == 0) %>% # important! it should not count zeros
  count(isRetweet, variable) %>%
  group_by(isRetweet, variable) %>%
  summarise(variable_sum = sum(n)) %>%
  arrange((variable)) %>%
  ungroup()

ba_mood <- ba_mood[c("variable", "isRetweet", "variable_sum")]

# Rename column values isRetweet = false -> TWEET, isRetweet = true -> RETWEET
ba_mood$isRetweet[ba_mood$isRetweet == "FALSE"]  <- c("TWEET")
ba_mood$isRetweet[ba_mood$isRetweet == "TRUE"]   <- c("RETWEET")

# Plot the final BANKS mood ring in extra high quality, vector graphics SVG format!
svg(file="../../img/charts-sentiment/ba-moodring.svg",width=5,height=5, pointsize=10)
# Colors are tried to define them accordingly to Plutchik´s color wheel (2001) to represent the emotions.
grid.col = c("TWEET" = "aliceblue", "RETWEET" = "aliceblue", "anger" = "orangered2", "anticipation" = "orange2", "disgust" = "mediumpurple2", "fear" = "green4", "joy" = "gold2", "sadness" = "deepskyblue2", "surprise" = "dodgerblue2", "trust" = "olivedrab3")

circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(ba_mood[[1]])) - 1), 15,
                         rep(5, length(unique(ba_mood[[2]])) - 1), 15))
chordDiagram(ba_mood, grid.col = grid.col, transparency = .2)
title("Banking: Emotional Relationships - Tweets and Retweets")
dev.off()

```

### Hypothesis testing H5. H5: Companies in the Swiss FinTech sector experience higher trust based on retweets to corporate tweets than Swiss banking companies.
```{r}

# 1. Stats properties
# 2. Testing for normal distribution (Shapiro-Wilk) for emotional dimension score: Trust. Both sectors, banks vs. FinTech
# 3. Testing via t-test or Wilcoxon
# 4. Results

# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-fintech-syu-nlp.Rdata')
# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-banks-syu-nlp.Rdata')

# Load ALL TWEETS, Capture retweets
ba_data_nrc_2 <- filter(ba_big_syu, isRetweet == "TRUE")
ft_data_nrc_syu_2 <- filter(ft_big_syu, isRetweet == "TRUE")

# Check only retweets, banks and FinTech
ba_data_nrc
ft_data_nrc_syu

# 1. Stats properties
describe(ba_data_nrc$trust)
describe(ft_data_nrc_syu$trust)
hist(ba_data_nrc$trust)
hist(ft_data_nrc_syu$trust)

# 1. Stats properties
describe(ba_data_nrc_2$trust)
describe(ft_data_nrc_syu_2$trust)
hist(ba_data_nrc_2$trust)
hist(ft_data_nrc_syu_2$trust)


# 2. Shapiro test for normal distribution
with(ba_data_nrc, shapiro.test(ba_data_nrc$trust))
with(ft_data_nrc_syu, shapiro.test(ft_data_nrc_syu$trust))

with(ba_data_nrc_2, shapiro.test(ba_data_nrc_2$trust))
with(ft_data_nrc_syu_2, shapiro.test(ft_data_nrc_syu_2$trust))

# 3. Not normal distributed, so Wilcoxon rank-sum test
# Write types into dataset, merge them
ba_data_nrc$type <- rep("Bank", length(ba_data_nrc$trust))
ft_data_nrc_syu$type <- rep("Fintech", length(ft_data_nrc_syu$trust))
ba_ft_syu_wcox <- rbind(ba_data_nrc, ft_data_nrc_syu)

# 4. Wilcoxon ranked-sum test
syu_trust_res_wilcox <- wilcox.test(trust ~ type, data = ba_ft_syu_wcox,
                   exact = FALSE, alternative="less")
syu_trust_res_wilcox
# p < 0.000

# Further stats properties
descr_ba <- describe(ba_data_nrc)
descr_ft <- describe(ft_data_nrc_syu)
descr_ba
descr_ft

# Only select needed data for box plot / H5 retweets
ba_ft_syu_wcox <- dplyr::select(ba_ft_syu_wcox, type, anger, anticipation, disgust, fear, joy, sadness, surprise, trust)
ba_ft_syu_wcox

# Data prep for mood ring - data melt
ba_ft_syu_wcox_melt <- melt(data = ba_ft_syu_wcox, id.vars = c("type"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

# Filter zeros, and emotional dim of trust
ba_ft_syu_wcox_melt <-  ba_ft_syu_wcox_melt %>%
  filter(!value == 0) %>%
  filter(variable=="trust")

# Chart: Boxplot Sentiment Score Compared to Entity Type Total
ggplot(ba_ft_syu_wcox_melt) +
  geom_boxplot(aes(x=variable, y=(value), col=as.factor(type))) +
  labs(x = "Sentiment", y = "Sentiment score",
       title="Sentiment Score of Trust Retweets, Compared to Entity Type")+
  scale_color_discrete(name="Entity Type")+
  coord_flip()



```

### Hypothesis testing H5. Bonus: Mood ring of tweets compared to responses (8 emotions of tweets put in relaton to 8 emotions of responses)
### Sector: BANKING
### Hint: http://www.r-graph-gallery.com/123-circular-plot-circlize-package-2/
### Readme: https://jokergoo.github.io/circlize_book/book/the-chorddiagram-function.html
```{r}
# LOAD again whole Bank dataset with cleaned tweets and syuzhet NLP results
load('../../data/output/smm-banks-fintech-output-banks-syu-nlp.Rdata')
ba_big_syu

# Filter bank tweets and responses
ba_resp <- filter(ba_big_syu, id %in% replyToSID | replyToSID %in% id)

# Banks
# Filter and divide into bank and clients communication
# ba_resp_banks <- filter(ba_resp, screenName %in% tw_parse) # communication banks
# ba_resp_clients <- filter(ba_resp, !screenName %in% tw_parse) # communication clients

# Select only the relevant fields: emotional dimensions, id, parent id
ba_resp <- dplyr::select(ba_resp, anger, anticipation, disgust, fear, joy, sadness, surprise, trust, replyToSID, id)

# Remove duplicates
ba_resp = unique(ba_resp)

# Melting data for reshaping it into a better processable format
ba_resp_melt <- melt(data = ba_resp, id.vars = c("replyToSID", "id"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

# Cleaning of tweets classified with only zeros.
# ba_resp <- filter(ba_resp, !anger == 0 | !anticipation == 0 | !disgust == 0 | !fear == 0 | !joy == 0 | !sadness == 0 | !surprise == 0 | !trust == 0)

## An adjacency list looks like this:
##   from to value
## 1    a  A     1
## 2    b  B     2
## 3    c  C     3

# Now trying to convert the tweet and response relations into an adjacency list:
# Initialize an empty myAdjList
myAdjList <- list()
# Add special adj counter
adjcounter = 1
# Loops
for (i in 1:nrow(ba_resp_melt)) {
  print (i)
  
  for (j in 1:nrow(ba_resp_melt)) {
    #print(j)

    # Skip if replyToSID is NA
    if(is.na(ba_resp_melt$replyToSID[j])) next

    # Loop through the dataset
    if(ba_resp_melt$id[i] == ba_resp_melt$replyToSID[j]) {
      # Emotion value from tweet to response
      a <- ba_resp_melt$variable[i] # Tweet
      b <- ba_resp_melt$variable[j] # Response
      c <- ba_resp_melt$value[i] # Emotion value of tweet
      
      a <- paste("T_", a, sep="") # Mark as Tweet
      b <- paste("R_", b, sep="") # Mark as Response
      # Add
      tmp_tw <- list(from=a, to=b, value=c)
      
      # Emotion value from response to tweet
      d <- ba_resp_melt$variable[j] # Response
      e <- ba_resp_melt$variable[i] # Tweet
      f <- ba_resp_melt$value[j] # Emotion value of response
      
      d <- paste("R_", d, sep="") # Mark as Response
      e <- paste("T_", e, sep="") # Mark as Tweet
      # Add
      tmp_re <- list(from=d, to=e, value=f)
      
      # Add records to adjacency list
      # Add tweet to response emotion value, if emotion value tweet c != 0 and emotion value response f != 0
      if (c != 0 && f != 0) {
        myAdjList[[adjcounter]] <- tmp_tw
        adjcounter = adjcounter + 1
      }
      
      # Add response to tweet value, if emotion value response f != 0 and emotion value tweet c != 0
      if (f != 0 && c != 0) {
        myAdjList[[adjcounter]] <- tmp_re
        adjcounter = adjcounter + 1
      }
      
    }
  }
}

# Convert list to dataframe
ba_df <- dplyr::bind_rows(myAdjList)

# SAVE ba_df - tweets to responses relations
save(ba_df, file = '../../data/output/smm-banks-fintech-output-banks-responses.Rdata')
# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-banks-responses.Rdata')

# Aggregate and group results
ba_resp_mood <- ba_df %>%
  filter(!value == 0) %>% # important! it should not count zeros
  #count(from, to) %>%
  group_by(from, to) %>%
  summarise(value = sum(value)) %>%
  arrange(from, to) %>%
  ungroup()

# Reduced to Lvl1 complexity (only showing retweets to corporate tweets, not the other direction). The more complex mood ring (with all levels of complexity is moved to the appendix in the thesis).
ba_resp_mood <- ba_resp_mood %>%
  filter(str_detect(from, "^R_"))


# Plot the FINAL BANKS RESPONSES MOOD RING in extra high quality, vector graphics SVG format!
svg(file="../../img/charts-sentiment/ba-responses-moodring.svg",width=5,height=5, pointsize=10)
# Colors are tried to define them accordingly to Plutchik´s color wheel (2001) to represent the emotions.
grid.col = c("T_anger" = "orangered2", "T_anticipation" = "orange2", "T_disgust" = "mediumpurple2", "T_fear" = "green4", "T_joy" = "gold2", "T_sadness" = "deepskyblue2", "T_surprise" = "dodgerblue2", "T_trust" = "olivedrab3",
             "R_anger" = "orangered2", "R_anticipation" = "orange2", "R_disgust" = "mediumpurple2", "R_fear" = "green4", "R_joy" = "gold2", "R_sadness" = "deepskyblue2", "R_surprise" = "dodgerblue2", "R_trust" = "olivedrab3")

circos.clear()
#Set the gap size
circos.par(gap.after = 2)
chordDiagram(ba_resp_mood, grid.col = grid.col, transparency = .2)
title("Banking: Emotional Relationships - Responses to Tweets")
dev.off()

```

### Hypothesis testing H5. Bonus: Mood ring of tweets compared to responses (8 emotions of tweets put in relaton to 8 emotions of responses)
### Sector: FINTECH
### Hint: http://www.r-graph-gallery.com/123-circular-plot-circlize-package-2/
### Readme: https://jokergoo.github.io/circlize_book/book/the-chorddiagram-function.html
```{r}
# LOAD again whole FinTech dataset with cleaned tweets and syuzhet NLP results
load('../../data/output/smm-banks-fintech-output-fintech-syu-nlp.Rdata')
ft_big_syu

# Filter bank and fintech tweets and responses
ft_resp <- filter(ft_big_syu, id %in% replyToSID | replyToSID %in% id)

# Fintech
# Filter and divide into bank and clients communication
# ft_resp_banks <- filter(ft_resp, screenName %in% tw_parse) # communication banks
# ft_resp_clients <- filter(ft_resp, !screenName %in% tw_parse) # communication clients

# Select only the relevant fields: emotional dimensions, id, parent id
ft_resp <- dplyr::select(ft_resp, anger, anticipation, disgust, fear, joy, sadness, surprise, trust, replyToSID, id)

# Remove duplicates
ft_resp = unique(ft_resp)

# Melting data for reshaping it into a better processable format
ft_resp_melt <- melt(data = ft_resp, id.vars = c("replyToSID", "id"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

# Cleaning of tweets classified with only zeros.
# ft_resp <- filter(ft_resp, !anger == 0 | !anticipation == 0 | !disgust == 0 | !fear == 0 | !joy == 0 | !sadness == 0 | !surprise == 0 | !trust == 0)

## An adjacency list looks like this:
##   from to value
## 1    a  A     1
## 2    b  B     2
## 3    c  C     3

# Now trying to convert the tweet and response relations into an adjacency list:
# Initialize an empty myAdjList
myAdjList <- list()
# Add special adj counter
adjcounter = 1
# Loops
for (i in 1:nrow(ft_resp_melt)) {
  print (i)
  
  for (j in 1:nrow(ft_resp_melt)) {
    #print(j)

    # Skip if replyToSID is NA
    if(is.na(ft_resp_melt$replyToSID[j])) next

    # Loop through the dataset
    if(ft_resp_melt$id[i] == ft_resp_melt$replyToSID[j]) {
      # Emotion value from tweet to response
      a <- ft_resp_melt$variable[i] # Tweet
      b <- ft_resp_melt$variable[j] # Response
      c <- ft_resp_melt$value[i] # Emotion value of tweet
      
      a <- paste("T_", a, sep="") # Mark as Tweet
      b <- paste("R_", b, sep="") # Mark as Response
      # Add
      tmp_tw <- list(from=a, to=b, value=c)
      
      # Emotion value from response to tweet
      d <- ft_resp_melt$variable[j] # Response
      e <- ft_resp_melt$variable[i] # Tweet
      f <- ft_resp_melt$value[j] # Emotion value of response
      
      d <- paste("R_", d, sep="") # Mark as Response
      e <- paste("T_", e, sep="") # Mark as Tweet
      # Add
      tmp_re <- list(from=d, to=e, value=f)
      
      # Add records to adjacency list
      # Add tweet to response emotion value, if emotion value tweet c != 0
      if (c != 0 && f != 0) {
        myAdjList[[adjcounter]] <- tmp_tw
        adjcounter = adjcounter + 1
      }
      
      # Add response to tweet value, if emotion value response f != 0
      if (f != 0 && c != 0) {
        myAdjList[[adjcounter]] <- tmp_re
        adjcounter = adjcounter + 1
      }
      
    }
  }
}

# Convert list to dataframe
ft_df <- dplyr::bind_rows(myAdjList)

# SAVE ba_df - tweets to responses relations
save(ft_df, file = '../../data/output/smm-banks-fintech-output-fintech-responses.Rdata')
# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-fintech-responses.Rdata')

# Aggregate and group results
ft_resp_mood <- ft_df %>%
  filter(!value == 0) %>% # important! it should not count zeros
  #count(from, to) %>%
  group_by(from, to) %>%
  summarise(value = sum(value)) %>%
  arrange(from, to) %>%
  ungroup()

# Reduced to Lvl1 complexity (only showing retweets to corporate tweets, not the other direction). The more complex mood ring (with all levels of complexity is moved to the appendix in the thesis).
ft_resp_mood <- ft_resp_mood %>%
  filter(str_detect(from, "^R_"))

# Plot the FINAL BANKS RESPONSES MOOD RING in extra high quality, vector graphics SVG format!
svg(file="../../img/charts-sentiment/ft-responses-moodring.svg",width=5,height=5, pointsize=10)
# Colors are tried to define them accordingly to Plutchik´s color wheel (2001) to represent the emotions.
grid.col = c("T_anger" = "orangered2", "T_anticipation" = "orange2", "T_disgust" = "mediumpurple2", "T_fear" = "green4", "T_joy" = "gold2", "T_sadness" = "deepskyblue2", "T_surprise" = "dodgerblue2", "T_trust" = "olivedrab3",
             "R_anger" = "orangered2", "R_anticipation" = "orange2", "R_disgust" = "mediumpurple2", "R_fear" = "green4", "R_joy" = "gold2", "R_sadness" = "deepskyblue2", "R_surprise" = "dodgerblue2", "R_trust" = "olivedrab3")

circos.clear()
#Set the gap size
circos.par(gap.after = 2)
chordDiagram(ft_resp_mood, grid.col = grid.col, transparency = .2)
title("FinTech: Emotional Relationships - Responses to Tweets")
dev.off()

# --- END ---

# Circlize plot example
#Create data
name=c(3,10,10,3,6,7,8,3, 3)
feature=paste("feature ", c(1,1,2,2,3,3,4,4, 4) , sep="")
dat <- data.frame(name,feature)
dat <- with(dat, table(name, feature))
dat

circos.clear() 
# Make the circular plot
chordDiagram(as.data.frame(dat), transparency = 0.5)
title("Example")

```

### Hypothesis testing H5 BONUS. H5: Companies in the Swiss FinTech sector experience higher trust based on responses to corporate tweets than Swiss banking companies.
```{r}
# Now the responses of trust are compared.
# 1. Stats properties
# 2. Testing for normal distribution (Shapiro-Wilk) for emotional dimension score: Trust. Both sectors, banks vs. FinTech
# 3. Testing via t-test or Wilcoxon
# 4. Results
# 5. Plotting boxplot

# LOAD again (banks responses)
load('../../data/output/smm-banks-fintech-output-banks-responses.Rdata')
# LOAD again (fintech responses)
load('../../data/output/smm-banks-fintech-output-fintech-responses.Rdata')

# Aggregate and group results banks
ba_resp_mood <- ba_df %>%
  filter(from == "R_trust")  # Only take trust responses for histogram and hypothesis testing, no aggregate function
#  group_by(from, to) %>%
#  summarise(value = sum(value)) %>%
#  arrange(from, to) %>%
#  ungroup()

# Aggregate and group results fintech
ft_resp_mood <- ft_df %>%
  filter(from == "R_trust")  # Only take trust responses for histogram and hypothesis testing, no aggregate function
# group_by(from, to) %>%
# summarise(value = sum(value)) %>%
# arrange(from, to) %>%
# ungroup()


# 1. Stats properties
describe(ba_resp_mood$value)
describe(ft_resp_mood$value)
hist(ba_resp_mood$value)
hist(ft_resp_mood$value)

# Histogram banks with percentage
h_ba = hist(ba_resp_mood$value, plot = FALSE)# hist(x,plot=FALSE) to avoid the plot of the histogram
h_ba$density = h_ba$counts/sum(h_ba$counts)
plot(h_ba,freq=FALSE)

# Histogram fintech with percentage
h_ft = hist(ft_resp_mood$value, plot = FALSE)# hist(x,plot=FALSE) to avoid the plot of the histogram
h_ft$density = h_ft$counts/sum(h_ft$counts)
plot(h_ft,freq=FALSE)

# 2. Shapiro test for normal distribution
with(ba_resp_mood, shapiro.test(ba_resp_mood$value))
# W = 0.85003, p-value = 0.09534
with(ft_resp_mood, shapiro.test(ft_resp_mood$value))
# W = 0.82203, p-value = 0.04902

# 3. Not normal distributed, so Wilcoxon rank-sum test
# Write types into dataset, merge them
ba_resp_mood$type <- rep("Bank", length(ba_resp_mood$value))
ft_resp_mood$type <- rep("FinTech", length(ft_resp_mood$value))
ba_ft_resp_wcox <- rbind(ba_resp_mood, ft_resp_mood)

# 4. Wilcoxon ranked-sum test
ba_ft_resp_trust_wcox <- wilcox.test(value ~ type, data = ba_ft_resp_wcox,
                   exact = FALSE, alternative="less")

ba_ft_resp_trust_wcox
# W = 0, p-value = 0.0004455

# 5. Plots: plotting boxplots, showing responses
ba_df$type <- rep("Bank", length(ba_df$value))
ft_df$type <- rep("FinTech", length(ft_df$value))
ba_ft_resp_all <- rbind(ba_df, ft_df)

# Only responses in the boxplot
ba_ft_resp_all <- ba_ft_resp_all %>%
  filter(str_detect(from, "^R_trust"))

# Chart: Boxplot Sentiment Score Compared to Entity Type Total
ggplot(ba_ft_resp_all) +
  geom_boxplot(aes(x=from, y=(value), col=as.factor(type))) +
  labs(x = "Sentiment", y = "Sentiment score",
       title="Sentiment Score of Trust Responses, Compared to Entity Type")+
  scale_color_discrete(name="Entity Type")+
  coord_flip()



```
