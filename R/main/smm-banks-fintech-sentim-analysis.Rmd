---
title: "MAS-Thesis-SMA-Sentiment-Analysis"
author: "ajpucher"
date: "2018 M08 5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Inventory of all libs / load all libs here

```{r}
library(circlize) # Visuals, circle diagrams, citation needed: Gu, Z. circlize implements and enhances circular visualization in R. Bioinformatics 2014.
library(dplyr) # Data ops
library(ggplot2) # Visuals
library(ggrepel) # Visuals
library(googleLanguageR) # Google NLP lib
library(magrittr) # Forward pipe operator
library(mvnormtest) # Tests for multivariate normality
library(parallel) # Parallelization / multi-core computation usage
library(psych) # Stats
library(RColorBrewer) # Colors
library(reshape2) # Reshape data
library(scatterplot3d) # 3d plots, MLR
library(stringr) # String ops
library(syuzhet) # Stanford NLP lib, citation needed
library(tidytext) # Text mining
library(twitteR) # Twitter API lib



```

### Source / load necessary R functions

```{r}
# Current working dir
getwd()

# turn-off scientific notation like 1e+48
options(scipen=999)

# Load authentication functions
source("../functions/auth.R")
# Load utils
source("../functions/util.R")

```

### Authentication to Twitter API (works, July 2018)

```{r}
# Start Authentication to Twitter API
auth_twitter()
# Start Authentication to Google NLP API
auth_google()

```

### Starting sentiment analysis, try some first examples
```{r}

load('../../data/output/smm-banks-fintech-output-descr-ratio-melted.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

#Some examples

#Trends in ZRH, Switzerland
#return data frame with name, country & woeid. 
Locs <- availableTrendLocations()
# Where woeid is a numerical identification code describing a location ID
# Filter the data frame for Delhi (India) and extract the woeid of the same
LocsCH = subset(Locs, country == "Switzerland") 
woeidZurich = subset(LocsCH, name == "Zurich")$woeid
# getTrends takes a specified woeid and returns the trending topics associated with that woeid
trends = getTrends(woeid=woeidZurich)
trends

# Search tweets about "Swiss Bank in a specific time range"
head(searchTwitter('Swiss Bank', since='2015-08-01', until= '2018-10-01'))

# Search for ubsschweiz tweets
ubsschweiz_tweets <- searchTwitter("#ubsschweiz", n=50)

zkbch_tweets <- searchTwitter("zkb_ch", n=50, lang="de")
str(zkbch_tweets)

zkbch_tweets

# Get user timelines
tweets = userTimeline("ubsschweiz", n=500)
length(tweets)
tweetsDF_test <- twListToDF(tweets)
head(tweetsDF_test)
tweetsDF_test


```

## Tweet data loads from both sectors
### Loading tweet data from FinTech sector, lang=en
### This chunk can run any number of times and cyclical to append additional data
```{r}
# Load again initial file (before norm and ratio calculations)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

# Load FinTech data from HDD for an n-th run to append additional data 
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')


d.file

# Filter type "Fintech"
df_sa_fintech <- filter(d.file, type == "Fintech")
df_sa_fintech


# Subselect account name for Twitter search "tw_parse"
df_sub_sa_ft <- subset(df_sa_fintech, select=c("tw_parse"))
df_sub_sa_ft <- na.omit(df_sub_sa_ft)


# Create list of dataframes of each account - FINTECH
fintech_list = list()

for (i in 1:nrow(df_sub_sa_ft)) {
  print (i)
  
  tweets <- searchTwitter(df_sub_sa_ft$tw_parse[i], n=500, lang="en")
  
  if (length(tweets) != 0) {
    tweetsDF <- twListToDF(tweets) # Convert to DF
    tweetsDF$tw_parse <- df_sub_sa_ft$tw_parse[i] # Keep track from which account parsed
    fintech_list[[i]] <- tweetsDF # Add it to your list
  }
  
}

fintech_list

# Bind rows - bind both dataframes into one, 1. read df from hdd, 2. read df by API, bind them in one, save.
fintech_data_all_tweets <- dplyr::bind_rows(fintech_list, fintech_data_all_tweets)
# Save
#save(fintech_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
save(fintech_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')

fintech_data_all_tweets

```

### Loading tweet data from banking sector, lang=de
### This chunk can run any number of times and cyclical to append additional data
```{r}
# Load again initial file (before norm and ratio calculations)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')
options(scipen=999)  # turn-off scientific notation like 1e+48

# Load Bank data from HDD for an n-th run to append additional data 
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')

# Filter type "Bank"
df_sa_banks <- filter(d.file, type == "Bank")
df_sa_banks

df_sub_sa_banks <- subset(df_sa_banks, select=c("tw_parse"))
df_sub_sa_banks <- na.omit(df_sub_sa_banks)

# Create list of dataframes of each account - BANKS
banks_list = list()

for (i in 1:nrow(df_sub_sa_banks)) {
  print (i)
  
  tweets <- searchTwitter(df_sub_sa_banks$tw_parse[i], n=500, lang="de")
  
  if (length(tweets) != 0) {
    tweetsDF <- twListToDF(tweets) # Convert to DF
    tweetsDF$tw_parse <- df_sub_sa_banks$tw_parse[i] # Keep track from which account parsed
    banks_list[[i]] <- tweetsDF # Add it to your list
  }
  
}

# Bind rows - bind both dataframes into one, 1. read df from hdd, 2. read df by API, bind them in one, save.
banks_data_all_tweets <- dplyr::bind_rows(banks_list, banks_data_all_tweets)
# Save
#save(banks_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')
save(banks_data_all_tweets, file = '../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

banks_data_all_tweets


```


## Prototyping sentiment analysis
### Cleaning fintech data
### References:  https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}
# Load data
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')

glimpse(fintech_data_all_tweets)

#Created in the first tutorial
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")

#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy <- fintech_data_all_tweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

glimpse(ft_data_tidy)

```

### Create sentiment datasets (bing, nrc, nrc polarity, afinn)
### References: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}
ft_data_bing <- ft_data_tidy %>%
  inner_join(get_sentiments("bing"))

ft_data_nrc <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc"))

ft_data_nrc_sub <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", "negative"))

ft_data_afinn <- ft_data_tidy %>%
  inner_join(get_sentiments("afinn"))

```

### Plotting first sentiment analysis results
### References: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
```{r}

# Plotting word count
nrc_plot <- ft_data_nrc %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  #Use `fill = -word_count` to make the larger bars darker
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) + #Turn off the legend
  labs(x = NULL, y = "Word Count") +
  scale_y_continuous(limits = c(0, 3000)) + #Hard code the axis limit
  ggtitle("Entity Type Fintech: Word Count - NRC Sentiment") +
  coord_flip()
nrc_plot

ggsave("../../img/charts-sent-analysis/chart1_s-a-nrc-word-count.png")

# Plotting NRC sentiments with words, which triggered those sentiments
plot_words <- ft_data_nrc %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(8)) %>% #consider top_n() from dplyr also
  ungroup()

plot_words %>%
  #Set `y = 1` to just plot one variable and use word as the label
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  #You want the words, not the points
  geom_point(color = "transparent") +
  #Make sure the labels don't overlap
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.04,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Entity Type Fintech: Trigger Words - NRC Sentiment") +
  coord_flip()

ggsave("../../img/charts-sent-analysis/chart2_s-a-nrc-words-sentiments.png")

# Plotting AFINN histogram
ggplot(data=ft_data_afinn, aes(x=ft_data_afinn$score)) + 
  geom_histogram(breaks=seq(-10, 10, by=1), 
                 col="black", 
                 fill="blue", 
                 alpha = .2) + 
  labs(title="Entity Type Fintech: Histogram AFINN Senti-Score", x="Score", y="Frequency")

ggsave("../../img/charts-sent-analysis/chart3_s-a-afinn-hist-senti-score.png")

# SD, Mean, Median
ft_hist_afinn <- c(sd(ft_data_afinn$score), mean(ft_data_afinn$score), median(ft_data_afinn$score))
ft_hist_afinn


```


## H2 testing. H2: Companies in the Swiss FinTech sector have a higher social impact in comparison of followers to retweets than Swiss banking companies.
### References: Adapted from Garcia (2018), Social Data Science Lecture
```{r}
# Load the saved file (the one with Twitter metadata)
load('../../data/output/smm-banks-fintech-output-descriptives.Rdata')

d.file
# Cleaning: Only remove row, if tw_parse = NA
d.file <- d.file[!is.na(d.file$tw_parse), ]

# Add new avg. retweet column
d.file$avgRT <- rep(NA, length(d.file$tw_parse))

# Calculate for every entity (bank or fintech) the avg. amount of retweets and write it to a new column
### Attention for the Twitter API rate limit! If hit, wait and run again.
i = 1
for (ent in d.file$tw_parse) {
  print(i)
  tweets = try(userTimeline(d.file[i, 'tw_parse'], n = 100))
  if (length(tweets)<10)
    { d.file$avgRT[i] <- NA }
  else
    { tweetsDF <- twListToDF(tweets)
      d.file$avgRT[i] <- mean(tweetsDF$retweetCount)
    }
  i <- i+1
}

df_avgRT <- d.file

# Testing
tweets = try(userTimeline("ecollect", n = 100))
tweetsDF2<-twListToDF(tweets)
mean(tweetsDF2$retweetCount)

# Cleaning 2: Only remove row, if avgRT = NA
df_avgRT <- df_avgRT[!is.na(df_avgRT$avgRT), ]

# Save
#save(df_avgRT, file = '../../data/output/smm-banks-fintech-output-descr-avg-retweets.Rdata')
save(df_avgRT, file = '../../data/output/smm-banks-fintech-output-descr-avg-retweets2.Rdata')

```


### H2: Display histograms, build regression models for banking and fintech
### Interpreting beta coefficients, linear regression etc. http://r-statistics.co/Linear-Regression.html
```{r}
# Load the saved file (with avgRT - average retweets)
load('../../data/output/smm-banks-fintech-output-descr-avg-retweets.Rdata')

df_avgRT

# Filter type "Fintech"
df_avgRT_Fintech <- filter(df_avgRT, type == "Fintech")
df_avgRT_Fintech

# Filter type "Bank"
df_avgRT_Bank <- filter(df_avgRT, type == "Bank")
df_avgRT_Bank

# Show histogram on log scale, with Twitter followers
hist(log(df_avgRT_Bank$tw_followers))
hist(log(df_avgRT_Fintech$tw_followers))

# Descriptive stats
describe(df_avgRT_Bank$tw_followers)
describe(df_avgRT_Fintech$tw_followers)

# Plots
plot(log(df_avgRT_Bank$tw_followers), log(df_avgRT_Bank$avgRT), xlab="Log followers", ylab="Log SI")
plot(log(df_avgRT_Fintech$tw_followers), log(df_avgRT_Fintech$avgRT), xlab="Log followers", ylab="Log SI")

# Clean for zeros because of logarithm
df_avgRT_Bank <-df_avgRT_Bank[df_avgRT_Bank$avgRT>0 & df_avgRT_Bank$tw_followers>0,]
df_avgRT_Fintech <-df_avgRT_Fintech[df_avgRT_Fintech$avgRT>0 & df_avgRT_Fintech$tw_followers>0,]

# Banks
df_avgRT_Bank$SI <- log(df_avgRT_Bank$avgRT)
df_avgRT_Bank$FC <- log(df_avgRT_Bank$tw_followers)
si_model_bank <- lm(SI ~ FC, data=df_avgRT_Bank)
summary(si_model_bank)


plot(log(df_avgRT_Bank$tw_followers), log(df_avgRT_Bank$avgRT), xlab="Log followers", ylab="Log SI", main = "Banks")+
abline(si_model_bank$coefficients[1], si_model_bank$coefficients[2], col="red")

# Fintech
df_avgRT_Fintech$SI <- log(df_avgRT_Fintech$avgRT)
df_avgRT_Fintech$FC <- log(df_avgRT_Fintech$tw_followers)
si_model_fintech <- lm(SI ~ FC, data=df_avgRT_Fintech)
summary(si_model_fintech)

plot(log(df_avgRT_Fintech$tw_followers), log(df_avgRT_Fintech$avgRT), xlab="Log followers", ylab="Log SI", main = "FinTech")+
abline(si_model_fintech$coefficients[1], si_model_fintech$coefficients[2], col="red")


```


## H3 Testing: H3: Tweet length has an impact to the number of retweets and likes to companies in the Swiss banking and Swiss FinTech sector.
### References:
### https://www.statmethods.net/graphs/scatterplot.html
### http://www.sthda.com/english/wiki/manova-test-in-r-multivariate-analysis-of-variance
### https://www.statmethods.net/stats/anova.html
### https://www.statmethods.net/stats/anovaAssumptions.html
```{r}
# Here follows H3 testing. Measurement variables are: a. Message length, b. Message likes, c. Message retweets.
# 1. Corporate messages (tweets) will be collected, from both sectors Banks and FinTech.
# 2. Two each message a column will be added: Measurement variable a.: Message length
# 3. Two MANOVA (with 2 DVs, 1 IV) regression models will be build, for both sectors Banks and FinTech.

# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')
# Load Bank data from HDD
#load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

# 1. Retweets will be collected, from both sectors Banks and FinTech.
# Filter for retweets
# FINTECH
fintech_data_all_tweets
fintech_data_only_tweets <- filter(fintech_data_all_tweets, isRetweet == "FALSE")

# BANKS
banks_data_all_tweets
banks_data_only_tweets <- filter(banks_data_all_tweets, isRetweet == "FALSE")

# Add column msg / text length
fintech_data_only_tweets$text_length <- nchar(fintech_data_only_tweets$text)
banks_data_only_tweets$text_length <- nchar(banks_data_only_tweets$text)

# Descriptive stats
describe(fintech_data_only_tweets$text_length)
describe(banks_data_only_tweets$text_length)

# QQ-plots for showing correlation
qqplot(fintech_data_only_tweets$text_length, fintech_data_only_tweets$favoriteCount)
qqplot(fintech_data_only_tweets$text_length, fintech_data_only_tweets$retweetCount)
qqplot(banks_data_only_tweets$text_length, banks_data_only_tweets$favoriteCount)
qqplot(banks_data_only_tweets$text_length, banks_data_only_tweets$retweetCount)

# Histograms about text_length
hist(fintech_data_only_tweets$text_length)
hist(banks_data_only_tweets$text_length)

# Validation for multivariate tests, Shapiro-Wilk test
# BANKS
# Subsetting the 2 DVs
ba_shap_dvs <- dplyr::select(banks_data_only_tweets, favoriteCount, retweetCount)
ba_shap_dvs <- as.matrix(ba_shap_dvs) # n x p numeric matrix of the 2 DVs

# Test Multivariate Normality for the 2 DVs
M <- t(ba_shap_dvs)
mshapiro.test(M)
# p < 0.00

# FINTECH
# Subsetting the 2 DVs
ft_shap_dvs <- dplyr::select(fintech_data_only_tweets, favoriteCount, retweetCount)
ft_shap_dvs <- as.matrix(ft_shap_dvs) # n x p numeric matrix of the 2 DVs

# Test Multivariate Normality for the 2 DVs
M <- t(ft_shap_dvs)
mshapiro.test(M)
# p < 0.00

# Do the MANOVA test, FINTECH
ft_res_man <- manova(cbind(favoriteCount, retweetCount) ~ text_length, data = fintech_data_only_tweets)
summary(ft_res_man)
# Investigate which variables are significant
summary.aov(ft_res_man)

# Do the MANOVA test, BANKS
ba_res_man <- manova(cbind(favoriteCount, retweetCount) ~ text_length, data = banks_data_only_tweets)
summary(ba_res_man)
# Investigate which variables are significant
summary.aov(ba_res_man)

# Plotting
# 3D plot FINTECH
attach(fintech_data_only_tweets) 
s3d_ft <-scatterplot3d(text_length, favoriteCount, retweetCount, pch=16, highlight.3d=TRUE,
  type="h", main="3D Scatterplot FinTech", , xlab ="Tweet length", ylab = "Favorites", zlab = "Retweets", col.axis = "gray", col.grid = "gray")

# 3D plot BANKS
attach(banks_data_only_tweets) 
s3d_ba <-scatterplot3d(text_length, favoriteCount, retweetCount, pch=16, highlight.3d=TRUE,
  type="h", main="3D Scatterplot Banks", , xlab ="Tweet length", ylab = "Favorites", zlab = "Retweets", col.axis = "gray", col.grid = "gray")



```


## H4 Testing: H4: Companies in the Swiss FinTech sector have a higher polarity score based on responses to corporate tweets than Swiss banking companies.
### References: 
### CRAN package, R libs to access Google NLP AP: https://cran.r-project.org/web/packages/googleLanguageR/vignettes/nlp.html
### Data pre-cleaning: eBook, Mastering Social Media Mining in R (Ravindran, 2015), Cleaning the corpus
### Cloud Natural Language by Google: https://cloud.google.com/natural-language/, https://taylorwhitten.github.io/blog/Google_NLP_SentimentAnalysis
```{r}
# Here follows H4 testing. 
# Measurement variables are: a. Polarity / Sentiment score of msg / tweet in (positive, neutral, negative: -1.0 to 1.0)

# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')
# Load Bank data from HDD
#load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

# 1. Retweets will be collected, from both sectors Banks and FinTech.
# Filter for retweets
# FINTECH
fintech_data_all_tweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")

#BANKS
banks_data_all_tweets
banks_data_retweets <- filter(banks_data_all_tweets, isRetweet == "TRUE")

# 2. Pre-cleaning and removing repetitive data, only tweets >= 5 chars
# FINTECH
ft_retweets_clean <- cleanTweets(fintech_data_retweets$text)
ft_retweets_clean = unique(ft_retweets_clean)
ft_retweets_clean <- subset(ft_retweets_clean, nchar(ft_retweets_clean)>= 5)
head (ft_retweets_clean)

# BANKS
ba_retweets_clean <- cleanTweets(banks_data_retweets$text)
ba_retweets_clean = unique(ba_retweets_clean)
ba_retweets_clean <- subset(ba_retweets_clean, nchar(ba_retweets_clean)>= 5)
head (ba_retweets_clean)

# 3. Polarity score will be calculated based on that tweets, by the help of Google NLP, an ML cloud-based approach.
# Google NLP API call, only of method: "language.documents.analyzeSentiment:	Analyzes the sentiment of the provided text."
# FINTECH
ft_nlp_result <- gl_nlp(ft_retweets_clean, nlp_type = "analyzeSentiment")
ft_nlp_result
# SAVE FINTECH Google NLP results
save(ft_nlp_result, file = '../../data/output/smm-banks-fintech-output-fintech-google-nlp-scores.Rdata')

# BANKS
ba_nlp_result <- gl_nlp(ba_retweets_clean, nlp_type = "analyzeSentiment")
ba_nlp_result
# SAVE BANKS Google NLP results
save(ba_nlp_result, file = '../../data/output/smm-banks-fintech-output-banks-google-nlp-scores.Rdata')

# 4. Visualization of results / histograms
# FINTECH
ggplot(ft_nlp_result$documentSentiment, aes(x=score, y=magnitude) ) +
  geom_point(alpha = 1/5) +
  geom_count()+
  geom_smooth(method = "loess", span=0.75) +
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis FinTech")

ggplot(ft_nlp_result$documentSentiment, aes( x=score) ) +
  geom_density(alpha = 0.25)+
  xlab("Polarity")+
  ylab("Magnitude")+
  ggtitle(paste0("Polarity Analysis FinTech"))
  

ft_nlp_result$documentSentiment

hist(ft_nlp_result$documentSentiment$magnitude, 
     main="Magnitude FinTech", 
     xlab="Magnitude", 
     las=1, 
     breaks=5)

hist(ft_nlp_result$documentSentiment$score, 
     main="Score FinTech", 
     xlab="Score", 
     las=1, 
     breaks=5)

# Create dataframe w. results, FinTech
df_ft_nlp_res <- ft_nlp_result$documentSentiment
df_ft_nlp_res$type <- rep("FinTech", length(df_ft_nlp_res$score))
# Additionally, add the retweets
df_ft_nlp_res$retweet <- ft_retweets_clean
df_ft_nlp_res

# BANKS
ggplot(ba_nlp_result$documentSentiment, aes(x=score, y=magnitude) ) +
  geom_point(alpha = 1/5) +
  geom_count()+
  geom_smooth(method = "loess", span=0.75) +
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis Banks")

ggplot(ba_nlp_result$documentSentiment, aes( x=score) ) +
  geom_density(alpha = 0.25)+
  xlab("Polarity")+
  ylab("Magnitude")+
  ggtitle(paste0("Polarity Analysis Banks"))
  

ba_nlp_result$documentSentiment

hist(ba_nlp_result$documentSentiment$magnitude, 
     main="Magnitude Banks", 
     xlab="Magnitude", 
     las=1, 
     breaks=5)

hist(ba_nlp_result$documentSentiment$score,
     main="Score Banks", 
     xlab="Score", 
     las=1, 
     breaks=5)

# Create dataframe w. results, Banks
df_ba_nlp_res <- ba_nlp_result$documentSentiment
df_ba_nlp_res$type <- rep("Banks", length(df_ba_nlp_res$score))
# Additionally, add the retweets
df_ba_nlp_res$retweet <- ba_retweets_clean

df_ba_nlp_res

# Create dataframe w. results, FinTech
df_nlp_plot_ba_ft <- rbind(df_ft_nlp_res, df_ba_nlp_res)
df_nlp_plot_ba_ft

# Creating FINAL H4 plot
ggplot(df_nlp_plot_ba_ft, aes(x=score, y=magnitude, fill = factor(type)) ) +
  geom_density(alpha = 0.25)+
  labs(x = "Score", y = "Magnitude",
       title="Polarity Analysis Banks and FinTech")+
  scale_fill_discrete(name="Entity Type")

# Chart: Boxplot Sentiment Score Compared to Entity Type Total
ggplot(df_nlp_plot_ba_ft) +
  geom_boxplot(aes(type, (score), col=as.factor(type))) +
  labs(x = "Type", y = "Sentiment score",
       title="Sentiment Score Compared to Entity Type")+
  scale_color_discrete(name="Entity Type")
  

# Using psych package for standard descriptive stats: https://www.statmethods.net/stats/descriptives.html
# Creating FINAL H4 descriptives
describe(ft_nlp_result$documentSentiment$score)
describe(ba_nlp_result$documentSentiment$score)

# Summary statistics
group_by(df_nlp_plot_ba_ft, type) %>%
  summarise(
    count = n(),
    mean = mean(score, na.rm = TRUE),
    median = median(score, na.rm = TRUE),
    sd = sd(score, na.rm = TRUE)
  )

# Shapiro test for normal distribution
with(df_nlp_plot_ba_ft, shapiro.test(score[type == "Banks"]))
with(df_nlp_plot_ba_ft, shapiro.test(score[type == "FinTech"]))

# Not normal distributed, so Wilcoxon rank-sum test
nlp_res_wilcox <- wilcox.test(score ~ type, data = df_nlp_plot_ba_ft,
                   exact = FALSE)

nlp_res_wilcox

```

## Testing H5. H5: Companies in the Swiss FinTech sector experience higher trust based on responses to corporate tweets than Swiss banking companies.
### References:  https://www.datacamp.com/community/tutorials/sentiment-analysis-R
### FinTech - sentiment analysis mostly in EN
```{r}
# FINTECH
# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')

# Load ALL TWEETS, Capture retweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")
glimpse(fintech_data_retweets)
```

### FinTech - Initially try the tidytext method
```{r}
# 1.A. Prep data to tidy text format, undertake sentiment analysis based on individual words, use tidytext libs
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy <- fintech_data_retweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

glimpse(ft_data_tidy)

# 2.A. TIDYTEXT: Undertake sentiment analysis with built-in tidytext nrc methodology
# Create sentiment dataset nrc
ft_data_nrc <- ft_data_tidy %>%
  inner_join(get_sentiments("nrc"))

ft_data_nrc

# 3.A. Plotting word emotion stats
ft_data_nrc %>%
  #filter(variable %in% "text") %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(x = NULL, y = "Word Count") +
  ggtitle("FinTech NRC Sentiment") +
  coord_flip()

ft_data_tidy$word

# 4.A. EXTRA Plotting
# Plotting NRC sentiments with words, which triggered those sentiments
plot_words <- ft_data_nrc %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(8)) %>% #consider top_n() from dplyr also
  ungroup()

plot_words %>%
  #Set `y = 1` to just plot one variable and use word as the label
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  #You want the words, not the points
  geom_point(color = "transparent") +
  #Make sure the labels don't overlap
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.04,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Entity Type Fintech: Trigger Words - NRC Sentiment") +
  coord_flip()

# MOOD RING plotting
# Calc. NRC for all tweets
# 1. Prep data to tidy text format, undertake sentiment analysis based on individual words, use tidytext libs
undesirable_words <- c(" ai ", " ca ", " la ", "hey", " na ", " da ", " uh ", " tin ", "  ll",  "la", "da", "uh", "ah")
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
ft_data_tidy_all <- fintech_data_all_tweets %>%
  unnest_tokens(word, text) %>% #Break the text into individual words
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words like "ah" or "oo"
  anti_join(stop_words) #Data provided by the tidytext package

# 2.TIDYTEXT: Undertake sentiment analysis with built-in tidytext nrc methodology
# Create sentiment dataset nrc
ft_data_nrc_all <- ft_data_tidy_all %>%
  inner_join(get_sentiments("nrc"))

# Plot the mood ring! :-)
#Define some colors to use throughout
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00", "#D65E00", "#6C5B7B", "#355C7D")
grid.col = c("TRUE" = "grey", "FALSE" = "grey", "anger" = my_colors[1], "anticipation" = my_colors[2], "disgust" = my_colors[3], "fear" = my_colors[4], "joy" = my_colors[5], "sadness" = my_colors[6], "surprise" = my_colors[7], "trust" = my_colors[8])

tweet_mood <-  ft_data_nrc_all %>%
  filter(!sentiment %in% c("positive", "negative")) %>%
  count(sentiment, isRetweet) %>%
  group_by(sentiment, isRetweet) %>%
  summarise(sentiment_sum = sum(n)) %>%
  ungroup()

circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(tweet_mood[[1]])) - 1), 15,
                         rep(5, length(unique(tweet_mood[[2]])) - 1), 15))
chordDiagram(tweet_mood, grid.col = grid.col, transparency = .2)
title("FinTech Sector: Relationship Between Mood and Tweets")
```

### FinTech - SYUZHET method (The SELECTED one), here starts the official H5 testing
```{r}

# FINTECH
# Load FinTech data from HDD
#load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-fintech-data-all-tweets2.Rdata')

# Load ALL TWEETS, Capture retweets
fintech_data_retweets <- filter(fintech_data_all_tweets, isRetweet == "TRUE")
glimpse(fintech_data_retweets)

# --- OTHER METHODOLOGY: SYUZHET ---
# 1.B. SYUZHET: Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ft_retweets_clean <- cleanTweets(fintech_data_retweets$text)
ft_retweets_clean = unique(ft_retweets_clean) # del redundant tweets
head (ft_retweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ft_data_nrc_syu <- get_nrc_sentiment(ft_retweets_clean, language = "english")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ft_data_nrc_syu <- get_nrc_sentiment(ft_retweets_clean, language = "english", cl=cl)
stopCluster(cl)

# 2.B. RETWEETS Plotting
# Plots, SENTENCES (whole tweets, sentences)
barplot(
  sort(colSums(prop.table(ft_data_nrc_syu[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1,
  col=c("white"),
  main = "Emotional dimensions - FinTech", xlab="Percentage"
)

# 3.B. EXTRA Plotting
# Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ft_alltweets_clean <- cleanTweets(fintech_data_all_tweets$text)
head (ft_alltweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ft_data_nrc_syu_all <- get_nrc_sentiment(ft_alltweets_clean, language = "english")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ft_data_nrc_syu_all <- get_nrc_sentiment(ft_alltweets_clean, language = "english", cl=cl)
stopCluster(cl)

# Merging to a big dataset, FinTech syuzhet
ft_big_syu <- cbind(ft_data_nrc_syu_all, ft_alltweets_clean)
ft_big_syu <- cbind(ft_big_syu, fintech_data_all_tweets)

# Further cleaning - DEL redundant records, if possible
ft_big_syu = unique(ft_big_syu) # del redundant tweets

# SAVE whole FinTech dataset with cleaned tweets and syuzhet NLP results
save(ft_big_syu, file = '../../data/output/smm-banks-fintech-output-fintech-syu-nlp.Rdata')
# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-fintech-syu-nlp.Rdata')

# Only select needed data for FinTech mood ring
ft_fin_syu <- dplyr::select(ft_big_syu, isRetweet, anger, anticipation, disgust, fear, joy, sadness, surprise, trust)
ft_fin_syu

# Data prep for mood ring - data melt
ft_melt_syu <- melt(data = ft_fin_syu, id.vars = c("isRetweet"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

# Data aggregation
ft_mood <-  ft_melt_syu %>%
  filter(!value == 0) %>% # important! it should not count zeros
  count(isRetweet, variable) %>%
  group_by(isRetweet, variable) %>%
  summarise(variable_sum = sum(n)) %>%
  arrange((variable)) %>%
  ungroup()

ft_mood <- ft_mood[c("variable", "isRetweet", "variable_sum")]

# Rename column values isRetweet = false -> TWEET, isRetweet = true -> RETWEET
ft_mood$isRetweet[ft_mood$isRetweet == "FALSE"]  <- c("TWEET")
ft_mood$isRetweet[ft_mood$isRetweet == "TRUE"]   <- c("RETWEET") 


# Plot the final FINTECH mood ring in extra high quality, vector graphics SVG format!
svg(file="../../img/charts-sentiment/ft-moodring.svg",width=5,height=5, pointsize=10)
# Colors are tried to define them accordingly to Plutchik´s color wheel (2001) to represent the emotions.
grid.col = c("TWEET" = "aliceblue", "RETWEET" = "aliceblue", "anger" = "orangered2", "anticipation" = "orange2", "disgust" = "mediumpurple2", "fear" = "green4", "joy" = "gold2", "sadness" = "deepskyblue2", "surprise" = "dodgerblue2", "trust" = "olivedrab3")

circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(ft_mood[[1]])) - 1), 15,
                         rep(5, length(unique(ft_mood[[2]])) - 1), 15))
chordDiagram(ft_mood, grid.col = grid.col, transparency = .2)
title("FinTech: Emotional Relationships - Tweets and Retweets")
dev.off()

```


### Banking - sentiment analysis mostly in DE
### References:
### tidytext, circlize: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
### syuzhet package (Stanford NLP): https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html
### German stopwords: https://www.inwt-statistics.de/blog-artikel-lesen/text-mining-part-2-making-of.html
```{r}
# BANK
# Load Bank data from HDD
#load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets.Rdata')
load('../../data/output/smm-banks-fintech-output-banks-data-all-tweets2.Rdata')

# Load ALL TWEETS, Capture retweets
banks_data_retweets <- filter(banks_data_all_tweets, isRetweet == "TRUE")

# 1. SYUZHET: Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ba_retweets_clean <- cleanTweets(banks_data_retweets$text)
ba_retweets_clean = unique(ba_retweets_clean) # del redundant tweets
head (ba_retweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ba_data_nrc <- get_nrc_sentiment(ba_retweets_clean, language = "german")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ba_data_nrc <- get_nrc_sentiment(ba_retweets_clean, language = "german", cl=cl)
stopCluster(cl)

# 2. Plotting
# Plots, SENTENCES (whole tweets, sentences)
barplot(
  sort(colSums(prop.table(ba_data_nrc[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1,
  col= c("white"),
  main = "Emotional dimensions - Banking", xlab="Percentage"
)


# 3.B. EXTRA Plotting
# Prep data to clean tweets (whole sentences), undertake sentiment analysis based on tweets
ba_alltweets_clean <- cleanTweets(banks_data_all_tweets$text)
head (ba_alltweets_clean)

# Call SYUZHET multilanguage sentiment analysis method, on cleaned tweets (SENTENCES)
#ba_data_nrc_syu_all <- get_nrc_sentiment(ba_alltweets_clean, language = "german")
# With PARALLELIZATION, cluster creating for 2 cores
cl <- makeCluster(2)
clusterExport(cl = cl, c("get_nrc_sentiment"))
ba_data_nrc_syu_all <- get_nrc_sentiment(ba_alltweets_clean, language = "german", cl=cl)
stopCluster(cl)

# Merging to a big dataset, Banks syuzhet
ba_big_syu <- cbind(ba_data_nrc_syu_all, ba_alltweets_clean)
ba_big_syu <- cbind(ba_big_syu, banks_data_all_tweets)

# Further cleaning - DEL redundant records, if possible
ba_big_syu = unique(ba_big_syu) # del redundant tweets

# SAVE whole Bank dataset with cleaned tweets and syuzhet NLP results
save(ba_big_syu, file = '../../data/output/smm-banks-fintech-output-banks-syu-nlp.Rdata')
# LOAD again (if needed in a later stage for further work)
load('../../data/output/smm-banks-fintech-output-banks-syu-nlp.Rdata')

# Only select needed data for Banks mood ring
ba_fin_syu <- dplyr::select(ba_big_syu, isRetweet, anger, anticipation, disgust, fear, joy, sadness, surprise, trust)
ba_fin_syu

# Data prep for mood ring - data melt
ba_melt_syu <- melt(data = ba_fin_syu, id.vars = c("isRetweet"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

# Data aggregation
ba_mood <-  ba_melt_syu %>%
  filter(!value == 0) %>% # important! it should not count zeros
  count(isRetweet, variable) %>%
  group_by(isRetweet, variable) %>%
  summarise(variable_sum = sum(n)) %>%
  arrange((variable)) %>%
  ungroup()

ba_mood <- ba_mood[c("variable", "isRetweet", "variable_sum")]

# Rename column values isRetweet = false -> TWEET, isRetweet = true -> RETWEET
ba_mood$isRetweet[ba_mood$isRetweet == "FALSE"]  <- c("TWEET")
ba_mood$isRetweet[ba_mood$isRetweet == "TRUE"]   <- c("RETWEET")

# Plot the final BANKS mood ring in extra high quality, vector graphics SVG format!
svg(file="../../img/charts-sentiment/ba-moodring.svg",width=5,height=5, pointsize=10)
# Colors are tried to define them accordingly to Plutchik´s color wheel (2001) to represent the emotions.
grid.col = c("TWEET" = "aliceblue", "RETWEET" = "aliceblue", "anger" = "orangered2", "anticipation" = "orange2", "disgust" = "mediumpurple2", "fear" = "green4", "joy" = "gold2", "sadness" = "deepskyblue2", "surprise" = "dodgerblue2", "trust" = "olivedrab3")

circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(ba_mood[[1]])) - 1), 15,
                         rep(5, length(unique(ba_mood[[2]])) - 1), 15))
chordDiagram(ba_mood, grid.col = grid.col, transparency = .2)
title("Banking: Emotional Relationships - Tweets and Retweets")
dev.off()

```

### Hypothesis testing H5. H5: Companies in the Swiss FinTech sector experience higher trust based on responses to corporate tweets than Swiss banking companies.
```{r}

# 1. Stats properties
# 2. Testing for normal distribution (Shapiro-Wilk) for emotional dimension score: Trust. Both sectors, banks vs. FinTech
# 3. Testing via t-test or Wilcoxon
# 4. Results

# Check only retweets, banks and FinTech
ba_data_nrc
ft_data_nrc_syu

# 1. Stats properties
describe(ba_data_nrc$trust)
describe(ft_data_nrc_syu$trust)
hist(ba_data_nrc$trust)
hist(ft_data_nrc_syu$trust)

# 2. Shapiro test for normal distribution
with(ba_data_nrc, shapiro.test(ba_data_nrc$trust))
with(ft_data_nrc_syu, shapiro.test(ft_data_nrc_syu$trust))

# 3. Not normal distributed, so Wilcoxon rank-sum test
# Write types into dataset, merge them
ba_data_nrc$type <- rep("bank", length(ba_data_nrc$trust))
ft_data_nrc_syu$type <- rep("fintech", length(ft_data_nrc_syu$trust))
ba_ft_syu_wcox <- rbind(ba_data_nrc, ft_data_nrc_syu)

# 4. Wilcoxon ranked-sum test
syu_trust_res_wilcox <- wilcox.test(trust ~ type, data = ba_ft_syu_wcox,
                   exact = FALSE, alternative="less")

syu_trust_res_wilcox
# p < 0.000

# Further stats properties
descr_ba <- describe(ba_data_nrc)
descr_ft <- describe(ft_data_nrc_syu)

```

### Hypothesis testing H5. Bonus: Mood ring of tweets compared to retweets (8 emotions of tweets compared to 8 emotions of retweets)
### Hint: http://www.r-graph-gallery.com/123-circular-plot-circlize-package-2/
### Readme: https://jokergoo.github.io/circlize_book/book/the-chorddiagram-function.html
```{r}
# LOAD again whole Bank dataset with cleaned tweets and syuzhet NLP results
load('../../data/output/smm-banks-fintech-output-banks-syu-nlp.Rdata')
# LOAD again whole FinTech dataset with cleaned tweets and syuzhet NLP results
load('../../data/output/smm-banks-fintech-output-fintech-syu-nlp.Rdata')

ba_big_syu
ft_big_syu

# Filter bank and fintech tweets and responses
ba_resp <- filter(ba_big_syu, id %in% replyToSID | replyToSID %in% id)
ft_resp <- filter(ft_big_syu, id %in% replyToSID | replyToSID %in% id)

# Banks
# Filter and divide into bank and clients communication
# ba_resp_banks <- filter(ba_resp, screenName %in% tw_parse) # communication banks
# ba_resp_clients <- filter(ba_resp, !screenName %in% tw_parse) # communication clients

# Select only the relevant fields: emotional dimensions, id, parent id
ba_resp <- dplyr::select(ba_resp, anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive, replyToSID, id)

# Melting data for reshaping it into a better processable format
ba_resp_melt <- melt(data = ba_resp, id.vars = c("replyToSID", "id"),
                    measure.vars =c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust", "negative", "positive"))

# Cleaning of tweets classified with only zeros.
# ba_resp <- filter(ba_resp, !anger == 0 | !anticipation == 0 | !disgust == 0 | !fear == 0 | !joy == 0 | !sadness == 0 | !surprise == 0 | !trust == 0)

## Adjacency list like this:
##   from to value
## 1    a  A     1
## 2    b  B     2
## 3    c  C     3

# TODO: Now trying to convert into a resulting adjacency list:
# Initialize an empty myAdjList
myAdjList <- list()
# Loops
for (i in 1:nrow(ba_resp_melt)) {
  print (i)
  
  for (j in 1:nrow(ba_resp_melt)) {
    print(j)
    if(ba_resp_melt$id[i] == ba_resp_melt$replyToSID[j]) {
      # Tweet emotion value to Retweet
      a <- ba_resp_melt$variable[i] # Tweet
      b <- ba_resp_melt$variable[j] # Retweet
      c <- ba_resp_melt$value[i]
      
      b <- paste("R_", b, sep="") # Mark as Retweet
      # Add
      tmp_tw <- list(from=a, to=b, value=c)
      
      # Retweet emotion value to Tweet
      d <- ba_resp_melt$variable[j] # Retweet
      e <- ba_resp_melt$variable[i] # Tweet
      f <- ba_resp_melt$value[j]
      
      d <- paste("R_", b, sep="") # Mark as Retweet
      # Add
      tmp_rw <- list(from=d, to=e, value=f)
      
      # Add to adjacency list
      myAdjList[[]] <- tmp_tw
      myAdjList[[]] <- tmp_rw
      
    }
  }
}

ba_resp_melt$id[1]==ba_resp_melt$replyToSID[4]

# TODO: Fintech
# Filter and divide into fintech and clients communication
ft_resp_ft <- filter(ft_resp, screenName %in% tw_parse) # communication banks
ft_resp_clients <- filter(ft_resp, !screenName %in% tw_parse) # communication clients

# TODO: Circlize plot
# Circlize plot example
#Create data
name=c(3,10,10,3,6,7,8,3, 3)
feature=paste("feature ", c(1,1,2,2,3,3,4,4, 4) , sep="")
dat <- data.frame(name,feature)
dat <- with(dat, table(name, feature))
 
dat

circos.clear() 
# Make the circular plot
chordDiagram(as.data.frame(dat), transparency = 0.5)
title("Example")





```

